{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE, ADASYN\n",
        "from collections import Counter\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('final_cleaned_dataset.csv')\n",
        "\n",
        "# Print original class distribution\n",
        "print(\"Original class distribution:\")\n",
        "print(df['PerformanceScore_Encoded'].value_counts())\n",
        "print()\n",
        "\n",
        "# Check for non-numeric columns\n",
        "numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
        "categorical_cols = df.select_dtypes(exclude=['number']).columns.tolist()\n",
        "\n",
        "print(f\"Numeric columns: {len(numeric_cols)}\")\n",
        "print(f\"Categorical columns: {len(categorical_cols)}\")\n",
        "\n",
        "if 'Position' in df.columns and 'Position' not in categorical_cols:\n",
        "    categorical_cols.append('Position')\n",
        "\n",
        "if 'PerformanceScore_Encoded' in numeric_cols:\n",
        "    numeric_cols.remove('PerformanceScore_Encoded')\n",
        "\n",
        "# Remove categorical columns that are already one-hot encoded (True/False values)\n",
        "true_categorical_cols = []\n",
        "for col in categorical_cols:\n",
        "    if df[col].nunique() > 2 or df[col].dtype == 'object':\n",
        "        true_categorical_cols.append(col)\n",
        "\n",
        "# Create preprocessor\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numeric_cols),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), true_categorical_cols)\n",
        "    ])\n",
        "\n",
        "# Separate features and target variable\n",
        "X = df.drop('PerformanceScore_Encoded', axis=1)\n",
        "y = df['PerformanceScore_Encoded']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Apply preprocessing\n",
        "X_train_scaled = preprocessor.fit_transform(X_train)\n",
        "X_test_scaled = preprocessor.transform(X_test)\n",
        "\n",
        "print(f\"Transformed X_train shape: {X_train_scaled.shape}\")\n",
        "\n",
        "print(\"Training set class distribution before oversampling:\")\n",
        "print(Counter(y_train))\n",
        "print()\n",
        "\n",
        "# Function to train SVM and evaluate\n",
        "def train_and_evaluate(X_train, y_train, X_test, y_test, model_name):\n",
        "    # Train an SVM classifier\n",
        "    svm = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True)\n",
        "    svm.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = svm.predict(X_test)\n",
        "\n",
        "    # Evaluate\n",
        "    print(f\"=== {model_name} ===\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print()\n",
        "\n",
        "# 1. Train model without oversampling\n",
        "print(\"Training SVM without oversampling...\")\n",
        "train_and_evaluate(X_train_scaled, y_train, X_test_scaled, y_test, \"No Oversampling\")\n",
        "\n",
        "# 2. Apply SMOTE oversampling\n",
        "print(\"Applying SMOTE oversampling...\")\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
        "print(\"Training set class distribution after SMOTE:\")\n",
        "print(Counter(y_train_smote))\n",
        "print()\n",
        "\n",
        "train_and_evaluate(X_train_smote, y_train_smote, X_test_scaled, y_test, \"SMOTE\")\n",
        "\n",
        "# 3. Apply ADASYN oversampling\n",
        "print(\"Applying ADASYN oversampling...\")\n",
        "adasyn = ADASYN(random_state=42)\n",
        "X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train_scaled, y_train)\n",
        "print(\"Training set class distribution after ADASYN:\")\n",
        "print(Counter(y_train_adasyn))\n",
        "print()\n",
        "\n",
        "train_and_evaluate(X_train_adasyn, y_train_adasyn, X_test_scaled, y_test, \"ADASYN\")\n",
        "\n",
        "# 4. Fine-tuned SVM with SMOTE (to reduce overfitting)\n",
        "print(\"Training fine-tuned SVM with SMOTE...\")\n",
        "svm_tuned = SVC(kernel='rbf', C=0.1, gamma='auto', probability=True)\n",
        "svm_tuned.fit(X_train_smote, y_train_smote)\n",
        "y_pred_tuned = svm_tuned.predict(X_test_scaled)\n",
        "\n",
        "print(\"=== Fine-tuned SVM with SMOTE ===\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_tuned))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_tuned))"
      ],
      "metadata": {
        "id": "4SAhr2GTZUmw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2caed25-fd7e-47f4-f507-7c6c67c8be5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original class distribution:\n",
            "PerformanceScore_Encoded\n",
            "3    243\n",
            "4     37\n",
            "2     18\n",
            "1     13\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Numeric columns: 27\n",
            "Categorical columns: 30\n",
            "Transformed X_train shape: (248, 53)\n",
            "Training set class distribution before oversampling:\n",
            "Counter({3: 194, 4: 30, 2: 14, 1: 10})\n",
            "\n",
            "Training SVM without oversampling...\n",
            "=== No Oversampling ===\n",
            "Confusion Matrix:\n",
            "[[ 2  1  0  0]\n",
            " [ 0  3  1  0]\n",
            " [ 0  0 49  0]\n",
            " [ 0  0  7  0]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       1.00      0.67      0.80         3\n",
            "           2       0.75      0.75      0.75         4\n",
            "           3       0.86      1.00      0.92        49\n",
            "           4       0.00      0.00      0.00         7\n",
            "\n",
            "    accuracy                           0.86        63\n",
            "   macro avg       0.65      0.60      0.62        63\n",
            "weighted avg       0.76      0.86      0.80        63\n",
            "\n",
            "\n",
            "Applying SMOTE oversampling...\n",
            "Training set class distribution after SMOTE:\n",
            "Counter({3: 194, 4: 194, 2: 194, 1: 194})\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== SMOTE ===\n",
            "Confusion Matrix:\n",
            "[[ 2  1  0  0]\n",
            " [ 0  4  0  0]\n",
            " [ 0  0 33 16]\n",
            " [ 0  0  4  3]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       1.00      0.67      0.80         3\n",
            "           2       0.80      1.00      0.89         4\n",
            "           3       0.89      0.67      0.77        49\n",
            "           4       0.16      0.43      0.23         7\n",
            "\n",
            "    accuracy                           0.67        63\n",
            "   macro avg       0.71      0.69      0.67        63\n",
            "weighted avg       0.81      0.67      0.72        63\n",
            "\n",
            "\n",
            "Applying ADASYN oversampling...\n",
            "Training set class distribution after ADASYN:\n",
            "Counter({3: 194, 1: 194, 2: 193, 4: 192})\n",
            "\n",
            "=== ADASYN ===\n",
            "Confusion Matrix:\n",
            "[[ 2  1  0  0]\n",
            " [ 0  4  0  0]\n",
            " [ 0  0 31 18]\n",
            " [ 0  0  3  4]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       1.00      0.67      0.80         3\n",
            "           2       0.80      1.00      0.89         4\n",
            "           3       0.91      0.63      0.75        49\n",
            "           4       0.18      0.57      0.28         7\n",
            "\n",
            "    accuracy                           0.65        63\n",
            "   macro avg       0.72      0.72      0.68        63\n",
            "weighted avg       0.83      0.65      0.71        63\n",
            "\n",
            "\n",
            "Training fine-tuned SVM with SMOTE...\n",
            "=== Fine-tuned SVM with SMOTE ===\n",
            "Confusion Matrix:\n",
            "[[ 2  1  0  0]\n",
            " [ 1  3  0  0]\n",
            " [ 0  0 22 27]\n",
            " [ 0  0  2  5]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.67      0.67      0.67         3\n",
            "           2       0.75      0.75      0.75         4\n",
            "           3       0.92      0.45      0.60        49\n",
            "           4       0.16      0.71      0.26         7\n",
            "\n",
            "    accuracy                           0.51        63\n",
            "   macro avg       0.62      0.64      0.57        63\n",
            "weighted avg       0.81      0.51      0.58        63\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Function to get the confusion matrix\n",
        "def get_confusion_matrix(y_true, y_pred):\n",
        "    return confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Getting confusion matrices for all methods\n",
        "# Without balancing\n",
        "svm_no_balance = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True)\n",
        "svm_no_balance.fit(X_train_scaled, y_train)\n",
        "y_pred_no_balance = svm_no_balance.predict(X_test_scaled)\n",
        "cm_no_balance = get_confusion_matrix(y_test, y_pred_no_balance)\n",
        "\n",
        "# SMOTE\n",
        "svm_smote = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True)\n",
        "svm_smote.fit(X_train_smote, y_train_smote)\n",
        "y_pred_smote = svm_smote.predict(X_test_scaled)\n",
        "cm_smote = get_confusion_matrix(y_test, y_pred_smote)\n",
        "\n",
        "# ADASYN\n",
        "svm_adasyn = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True)\n",
        "svm_adasyn.fit(X_train_adasyn, y_train_adasyn)\n",
        "y_pred_adasyn = svm_adasyn.predict(X_test_scaled)\n",
        "cm_adasyn = get_confusion_matrix(y_test, y_pred_adasyn)\n",
        "\n",
        "# Fine-tuned SVM with SMOTE\n",
        "svm_tuned = SVC(kernel='rbf', C=0.1, gamma='auto', probability=True)\n",
        "svm_tuned.fit(X_train_smote, y_train_smote)\n",
        "y_pred_tuned = svm_tuned.predict(X_test_scaled)\n",
        "cm_tuned = get_confusion_matrix(y_test, y_pred_tuned)\n",
        "\n",
        "# Creating visualization of confusion matrices\n",
        "plt.figure(figsize=(16, 12))\n",
        "\n",
        "# Define classes for labels\n",
        "classes = sorted(np.unique(y))\n",
        "titles = ['No Balancing', 'SMOTE', 'ADASYN', 'Fine-tuned SVM with SMOTE']\n",
        "confusion_matrices = [cm_no_balance, cm_smote, cm_adasyn, cm_tuned]\n",
        "\n",
        "# Normalize matrices for better visualization\n",
        "normalized_matrices = []\n",
        "for cm in confusion_matrices:\n",
        "    normalized_cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    normalized_matrices.append(normalized_cm)\n",
        "\n",
        "# Create subplots for each matrix\n",
        "for i, (cm, title) in enumerate(zip(normalized_matrices, titles)):\n",
        "    plt.subplot(2, 2, i + 1)\n",
        "\n",
        "    # Create heatmap\n",
        "    sns.heatmap(cm, annot=True, fmt='.2f', cmap='Blues',\n",
        "                xticklabels=classes, yticklabels=classes, cbar=False)\n",
        "\n",
        "    # Add title and axis labels\n",
        "    plt.title(title, fontsize=14)\n",
        "    plt.ylabel('Actual Class', fontsize=12)\n",
        "    plt.xlabel('Predicted Class', fontsize=12)\n",
        "\n",
        "    # Rotate axis labels for better readability\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.yticks(rotation=45)\n",
        "\n",
        "# Add main title\n",
        "plt.suptitle('Comparison of Confusion Matrices for Different Balancing Methods', fontsize=16)\n",
        "\n",
        "# Adjust spacing between subplots\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "\n",
        "# Save the image\n",
        "plt.savefig('Figure_5_1_confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n",
        "# Additionally: create alternative visualization with absolute values\n",
        "plt.figure(figsize=(16, 12))\n",
        "\n",
        "for i, (cm, title) in enumerate(zip(confusion_matrices, titles)):\n",
        "    plt.subplot(2, 2, i + 1)\n",
        "\n",
        "    # Create heatmap with absolute values\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=classes, yticklabels=classes, cbar=False)\n",
        "\n",
        "    plt.title(title, fontsize=14)\n",
        "    plt.ylabel('Actual Class', fontsize=12)\n",
        "    plt.xlabel('Predicted Class', fontsize=12)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.yticks(rotation=45)\n",
        "\n",
        "plt.suptitle('Comparison of Confusion Matrices (Absolute Values)', fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "plt.savefig('Figure_5_1_confusion_matrices_absolute.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "f2wgNHpSFNVz",
        "outputId": "255041f8-74a5-4ca9-f286-907069729265"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x1200 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x1200 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ADASYN**"
      ],
      "metadata": {
        "id": "09F3RE-BCU6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the original dataset\n",
        "df = pd.read_csv('final_cleaned_dataset.csv')\n",
        "\n",
        "print(f\"Original dataset shape: {df.shape}\")\n",
        "print(f\"Class distribution before ADASYN:\")\n",
        "print(df['PerformanceScore_Encoded'].value_counts())\n",
        "print(df['PerformanceScore_Encoded'].value_counts(normalize=True).round(3) * 100)\n",
        "\n",
        "# ========================\n",
        "# Create all required features with correct names\n",
        "# ========================\n",
        "\n",
        "# List of consensus features we need to make sure exist in the final dataset\n",
        "consensus_features = [\n",
        "    'DaysLateLast30_MinMax',\n",
        "    'EngagementSurvey_Scaled',\n",
        "    'Tenure_Years_Scaled',\n",
        "    'Absences_MinMax',\n",
        "    'Salary_Scaled',\n",
        "    'EmpSatisfaction_Scaled',\n",
        "    'Age_Scaled',\n",
        "    'EmploymentStatus_Encoded',\n",
        "    'Sex_Encoded',\n",
        "    'MaritalDesc_Married',\n",
        "    'RaceDesc_Black or African American',\n",
        "    'MaritalDesc_Single',\n",
        "    'Position_Production Technician I',\n",
        "    'Position_Production Technician II',\n",
        "    'RecruitmentSource_LinkedIn'\n",
        "]\n",
        "\n",
        "# Create the two missing Position features\n",
        "if 'Position' in df.columns:\n",
        "    df['Position_Production Technician I'] = (df['Position'] == 'Production Technician I').astype(float)\n",
        "    df['Position_Production Technician II'] = (df['Position'] == 'Production Technician II').astype(float)\n",
        "else:\n",
        "    print(\"Warning: 'Position' column not found. Creating dummy position columns.\")\n",
        "    df['Position_Production Technician I'] = 0.0\n",
        "    df['Position_Production Technician II'] = 0.0\n",
        "\n",
        "# Verify all other features exist or create them from raw data if needed\n",
        "# This handles cases where we need to transform raw features into expected formats\n",
        "\n",
        "# Check for _Scaled features and create them if they don't exist\n",
        "for feature in ['EngagementSurvey', 'Tenure_Years', 'Salary', 'EmpSatisfaction', 'Age']:\n",
        "    scaled_feature = f\"{feature}_Scaled\"\n",
        "    if scaled_feature not in df.columns and feature in df.columns:\n",
        "        # Apply Min-Max scaling to create _Scaled version\n",
        "        df[scaled_feature] = (df[feature] - df[feature].min()) / (df[feature].max() - df[feature].min())\n",
        "        print(f\"Created {scaled_feature} from {feature}\")\n",
        "\n",
        "# Check for binary categorical features and create them if needed\n",
        "if 'MaritalDesc' in df.columns:\n",
        "    if 'MaritalDesc_Married' not in df.columns:\n",
        "        df['MaritalDesc_Married'] = (df['MaritalDesc'] == 'Married').astype(float)\n",
        "        print(\"Created MaritalDesc_Married\")\n",
        "    if 'MaritalDesc_Single' not in df.columns:\n",
        "        df['MaritalDesc_Single'] = (df['MaritalDesc'] == 'Single').astype(float)\n",
        "        print(\"Created MaritalDesc_Single\")\n",
        "\n",
        "if 'RaceDesc' in df.columns and 'RaceDesc_Black or African American' not in df.columns:\n",
        "    df['RaceDesc_Black or African American'] = (df['RaceDesc'] == 'Black or African American').astype(float)\n",
        "    print(\"Created RaceDesc_Black or African American\")\n",
        "\n",
        "if 'RecruitmentSource' in df.columns and 'RecruitmentSource_LinkedIn' not in df.columns:\n",
        "    df['RecruitmentSource_LinkedIn'] = (df['RecruitmentSource'] == 'LinkedIn').astype(float)\n",
        "    print(\"Created RecruitmentSource_LinkedIn\")\n",
        "\n",
        "# Create encoded features if needed\n",
        "if 'Sex' in df.columns and 'Sex_Encoded' not in df.columns:\n",
        "    df['Sex_Encoded'] = df['Sex'].map({'F': 0, 'M': 1})\n",
        "    print(\"Created Sex_Encoded\")\n",
        "\n",
        "if 'EmploymentStatus' in df.columns and 'EmploymentStatus_Encoded' not in df.columns:\n",
        "    df['EmploymentStatus_Encoded'] = df['EmploymentStatus'].map({'Active': 1, 'Terminated': 0})\n",
        "    print(\"Created EmploymentStatus_Encoded\")\n",
        "\n",
        "# Create MinMax features if needed\n",
        "if 'Absences' in df.columns and 'Absences_MinMax' not in df.columns:\n",
        "    df['Absences_MinMax'] = (df['Absences'] - df['Absences'].min()) / (df['Absences'].max() - df['Absences'].min())\n",
        "    print(\"Created Absences_MinMax\")\n",
        "\n",
        "if 'DaysLateLast30' in df.columns and 'DaysLateLast30_MinMax' not in df.columns:\n",
        "    df['DaysLateLast30_MinMax'] = (df['DaysLateLast30'] - df['DaysLateLast30'].min()) / (df['DaysLateLast30'].max() - df['DaysLateLast30'].min())\n",
        "    print(\"Created DaysLateLast30_MinMax\")\n",
        "\n",
        "# ========================\n",
        "# Prepare data for ADASYN\n",
        "# ========================\n",
        "\n",
        "# Check which consensus features are actually available\n",
        "available_features = [col for col in consensus_features if col in df.columns]\n",
        "missing_features = [col for col in consensus_features if col not in df.columns]\n",
        "\n",
        "if missing_features:\n",
        "    print(f\"\\nWarning: The following features are still missing: {missing_features}\")\n",
        "    print(\"These features will not be used in the model.\")\n",
        "\n",
        "print(f\"\\nUsing {len(available_features)} consensus features: {available_features}\")\n",
        "\n",
        "# Separate target variable\n",
        "y = df['PerformanceScore_Encoded']\n",
        "X = df[available_features]  # Only use available consensus features\n",
        "\n",
        "# ========================\n",
        "# ADASYN Oversampling\n",
        "# ========================\n",
        "\n",
        "# Apply ADASYN\n",
        "n_neighbors = min(5, min(y.value_counts()) - 1)  # k_neighbors must be <= n_samples - 1\n",
        "print(f\"Using n_neighbors = {n_neighbors} for ADASYN\")\n",
        "\n",
        "adasyn = ADASYN(random_state=42, n_neighbors=n_neighbors)\n",
        "X_adasyn, y_adasyn = adasyn.fit_resample(X, y)\n",
        "\n",
        "print(f\"\\nADASYN dataset shape: {X_adasyn.shape}\")\n",
        "print(f\"Class distribution after ADASYN:\")\n",
        "unique, counts = np.unique(y_adasyn, return_counts=True)\n",
        "for class_label, count in zip(unique, counts):\n",
        "    print(f\"Class {class_label}: {count} ({count/len(y_adasyn)*100:.1f}%)\")\n",
        "\n",
        "# ========================\n",
        "# Create final dataframe for saving\n",
        "# ========================\n",
        "\n",
        "# Create DataFrame with the ADASYN-generated data\n",
        "adasyn_df = pd.DataFrame(X_adasyn, columns=available_features)\n",
        "adasyn_df['PerformanceScore_Encoded'] = y_adasyn\n",
        "\n",
        "# Save the ADASYN dataset to CSV\n",
        "adasyn_df.to_csv('employee_data_adasyn.csv', index=False)\n",
        "\n",
        "print(\"\\nADASYN dataset has been created and saved successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xuEOKxB_sh3",
        "outputId": "f44940bd-7bf9-4cd9-82c1-ccd576720736"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataset shape: (311, 57)\n",
            "Class distribution before ADASYN:\n",
            "PerformanceScore_Encoded\n",
            "3    243\n",
            "4     37\n",
            "2     18\n",
            "1     13\n",
            "Name: count, dtype: int64\n",
            "PerformanceScore_Encoded\n",
            "3    78.1\n",
            "4    11.9\n",
            "2     5.8\n",
            "1     4.2\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Using 15 consensus features: ['DaysLateLast30_MinMax', 'EngagementSurvey_Scaled', 'Tenure_Years_Scaled', 'Absences_MinMax', 'Salary_Scaled', 'EmpSatisfaction_Scaled', 'Age_Scaled', 'EmploymentStatus_Encoded', 'Sex_Encoded', 'MaritalDesc_Married', 'RaceDesc_Black or African American', 'MaritalDesc_Single', 'Position_Production Technician I', 'Position_Production Technician II', 'RecruitmentSource_LinkedIn']\n",
            "Using n_neighbors = 5 for ADASYN\n",
            "\n",
            "ADASYN dataset shape: (959, 15)\n",
            "Class distribution after ADASYN:\n",
            "Class 1: 241 (25.1%)\n",
            "Class 2: 240 (25.0%)\n",
            "Class 3: 243 (25.3%)\n",
            "Class 4: 235 (24.5%)\n",
            "\n",
            "ADASYN dataset has been created and saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model training**"
      ],
      "metadata": {
        "id": "nBGRzvHeDEWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM Training for Employee Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "'''# Upload the dataset in Google Colab\n",
        "uploaded = files.upload()  # This will prompt you to upload your CSV file\n",
        "file_name = list(uploaded.keys())[0]  # Get the filename of the uploaded file'''\n",
        "\n",
        "# Read the dataset\n",
        "df = pd.read_csv('employee_data_adasyn.csv')\n",
        "\n",
        "# Display basic information\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df.head())\n",
        "print(\"\\nData types:\")\n",
        "print(df.dtypes)\n",
        "print(\"\\nSummary statistics:\")\n",
        "print(df.describe())\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing values count:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Convert string True/False to boolean for relevant columns\n",
        "string_bool_columns = [col for col in df.columns if df[col].dtype == 'object' and\n",
        "                       set(df[col].unique()).issubset({\"True\", \"False\"})]\n",
        "for col in string_bool_columns:\n",
        "    df[col] = df[col].map({\"True\": True, \"False\": False})\n",
        "\n",
        "# Visualize distribution of potential target variables\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Assuming PerformanceScore_Encoded is the target variable\n",
        "df['PerformanceScore_Encoded'].value_counts().plot(kind='bar', ax=axes[0])\n",
        "axes[0].set_title('Distribution of Performance Scores')\n",
        "axes[0].set_xlabel('Performance Score')\n",
        "axes[0].set_ylabel('Count')\n",
        "\n",
        "# Employment Status distribution\n",
        "df['EmploymentStatus_Encoded'].value_counts().plot(kind='bar', ax=axes[1])\n",
        "axes[1].set_title('Distribution of Employment Status')\n",
        "axes[1].set_xlabel('Employment Status')\n",
        "axes[1].set_ylabel('Count')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Correlation analysis\n",
        "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(df[numeric_cols].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Define features and target variable - you can change the target as needed\n",
        "# Using PerformanceScore_Encoded as the target here\n",
        "X = df.drop('PerformanceScore_Encoded', axis=1)\n",
        "y = df['PerformanceScore_Encoded']\n",
        "\n",
        "# Identify numerical and categorical columns\n",
        "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_cols = X.select_dtypes(include=['bool', 'object']).columns.tolist()\n",
        "\n",
        "print(\"\\nNumerical columns:\", numerical_cols)\n",
        "print(\"Categorical columns:\", categorical_cols)\n",
        "\n",
        "# Create a preprocessing pipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_cols),\n",
        "        ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_cols)\n",
        "    ])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"\\nTraining set shape: {X_train.shape}, Testing set shape: {X_test.shape}\")\n",
        "\n",
        "# Create an SVM pipeline with preprocessing\n",
        "svm_pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', SVC(probability=True, random_state=42))\n",
        "])\n",
        "\n",
        "# Define hyperparameters for grid search\n",
        "param_grid = {\n",
        "    'classifier__C': [0.1, 1, 10, 100],\n",
        "    'classifier__gamma': ['scale', 'auto'],\n",
        "    'classifier__kernel': ['linear', 'rbf', 'poly']\n",
        "}\n",
        "\n",
        "# Perform grid search with cross-validation\n",
        "grid_search = GridSearchCV(svm_pipeline, param_grid, cv=5, scoring='accuracy', verbose=1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and best score\n",
        "print(\"\\nBest parameters:\", grid_search.best_params_)\n",
        "print(\"Best cross-validation score: {:.4f}\".format(grid_search.best_score_))\n",
        "\n",
        "# Use the best model to make predictions\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"\\nModel Evaluation:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion Matrix visualization\n",
        "plt.figure(figsize=(10, 8))\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save the best model (optional)\n",
        "import pickle\n",
        "with open('svm_employee_model.pkl', 'wb') as file:\n",
        "    pickle.dump(best_model, file)\n",
        "print(\"\\nModel saved as 'svm_employee_model.pkl'\")\n",
        "\n",
        "# Download the model (in Colab)\n",
        "files.download('svm_employee_model.pkl')\n",
        "\n",
        "# Feature importance analysis (using coefficients for linear kernel)\n",
        "if grid_search.best_params_['classifier__kernel'] == 'linear':\n",
        "    feature_names = numerical_cols.copy()\n",
        "    for name, ohe in preprocessor.transformers_:\n",
        "        if name == 'cat':\n",
        "            # Get the feature names from the one-hot encoder\n",
        "            feature_names.extend(ohe.get_feature_names_out(categorical_cols))\n",
        "\n",
        "    # Get the coefficients from the SVM model\n",
        "    coefficients = best_model.named_steps['classifier'].coef_\n",
        "\n",
        "    # For multiclass classification\n",
        "    if len(coefficients.shape) > 1:\n",
        "        # Take absolute value of coefficients and sum across classes\n",
        "        importance = np.sum(np.abs(coefficients), axis=0)\n",
        "    else:\n",
        "        importance = np.abs(coefficients[0])\n",
        "\n",
        "    # Create DataFrame for visualization\n",
        "    feature_importance = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\n",
        "    feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
        "\n",
        "    # Plot top 15 features\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.barplot(x='Importance', y='Feature', data=feature_importance.head(15))\n",
        "    plt.title('Top 15 Feature Importance (Linear SVM)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\nFeature importance analysis is only available for linear kernel\")"
      ],
      "metadata": {
        "id": "nVJQb5uVsD9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overfitting Detection**"
      ],
      "metadata": {
        "id": "0QYqtPEMC7SN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Overfitting Detection for SVM Employee Model\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import learning_curve, StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, roc_curve, auc, precision_recall_fscore_support,\n",
        "    roc_auc_score, classification_report, confusion_matrix\n",
        ")\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "import pickle\n",
        "from google.colab import files\n",
        "import io\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Upload the dataset and model in Google Colab\n",
        "print(\"Upload your CSV dataset first:\")\n",
        "uploaded_data = files.upload()  # Upload the dataset\n",
        "file_name = list(uploaded_data.keys())[0]\n",
        "\n",
        "print(\"\\nNow upload your saved SVM model:\")\n",
        "uploaded_model = files.upload()  # Upload the model\n",
        "model_name = list(uploaded_model.keys())[0]\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(io.BytesIO(uploaded_data[file_name]))\n",
        "\n",
        "# Convert string True/False to boolean for relevant columns\n",
        "string_bool_columns = [col for col in df.columns if df[col].dtype == 'object' and\n",
        "                       set(df[col].unique()).issubset({\"True\", \"False\"})]\n",
        "for col in string_bool_columns:\n",
        "    df[col] = df[col].map({\"True\": True, \"False\": False})\n",
        "\n",
        "# Define features and target variable\n",
        "X = df.drop('PerformanceScore_Encoded', axis=1)\n",
        "y = df['PerformanceScore_Encoded']\n",
        "\n",
        "# Load the trained model\n",
        "with open(model_name, 'rb') as file:\n",
        "    best_model = pickle.load(file)\n",
        "\n",
        "print(\"\\nModel loaded successfully!\")\n",
        "\n",
        "# Get the original data split that was used for training\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]}\")\n",
        "print(f\"Test set size: {X_test.shape[0]}\")\n",
        "\n",
        "# ========================= 1. Comparing Metrics on Training and Test Sets =========================\n",
        "def check_accuracy_gap():\n",
        "    # Get predictions on training and test sets\n",
        "    y_train_pred = best_model.predict(X_train)\n",
        "    y_test_pred = best_model.predict(X_test)\n",
        "\n",
        "    # Calculate accuracies\n",
        "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "    accuracy_gap = train_accuracy - test_accuracy\n",
        "\n",
        "    print(\"\\n1. ACCURACY GAP ANALYSIS\")\n",
        "    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "    print(f\"Accuracy Gap (): {accuracy_gap:.4f}\")\n",
        "\n",
        "    if accuracy_gap > 0.1:\n",
        "        print(\" Potential overfitting detected! ( > 0.1)\")\n",
        "    else:\n",
        "        print(\" No significant overfitting detected based on accuracy gap (  0.1)\")\n",
        "\n",
        "    # Visualization of accuracy comparison\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    bars = plt.bar(['Training', 'Test'], [train_accuracy, test_accuracy], color=['#3498db', '#2ecc71'])\n",
        "    plt.ylim(0, 1.1)\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Accuracy Comparison: Training vs Test')\n",
        "\n",
        "    # Add text labels above bars\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                f'{height:.4f}', ha='center', va='bottom')\n",
        "\n",
        "    # Add a horizontal line for the threshold\n",
        "    plt.axhline(y=train_accuracy - 0.1, color='r', linestyle='--', alpha=0.5)\n",
        "    plt.text(1.05, train_accuracy - 0.1, 'Threshold ( = 0.1)', color='r', va='center')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return train_accuracy, test_accuracy, accuracy_gap\n",
        "\n",
        "# ========================= 2. Learning Curves Analysis =========================\n",
        "def plot_learning_curves():\n",
        "    print(\"\\n2. LEARNING CURVES ANALYSIS\")\n",
        "\n",
        "    # Get the pipeline steps from the model\n",
        "    preprocessor = best_model.named_steps['preprocessor']\n",
        "    classifier = best_model.named_steps['classifier']\n",
        "\n",
        "    # Create a new pipeline for learning curve evaluation\n",
        "    pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', classifier)\n",
        "    ])\n",
        "\n",
        "    # Calculate learning curves\n",
        "    train_sizes = np.linspace(0.1, 1.0, 10)\n",
        "    train_sizes, train_scores, val_scores = learning_curve(\n",
        "        pipeline, X, y, train_sizes=train_sizes,\n",
        "        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
        "        scoring='accuracy', random_state=42, n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # Calculate mean and standard deviation for training set scores\n",
        "    train_mean = np.mean(train_scores, axis=1)\n",
        "    train_std = np.std(train_scores, axis=1)\n",
        "\n",
        "    # Calculate mean and standard deviation for validation set scores\n",
        "    val_mean = np.mean(val_scores, axis=1)\n",
        "    val_std = np.std(val_scores, axis=1)\n",
        "\n",
        "    # Calculate the gap between training and validation curves\n",
        "    gap = train_mean - val_mean\n",
        "\n",
        "    # NaN  \n",
        "    nan_mask = np.isnan(gap)\n",
        "    if np.any(nan_mask):\n",
        "        print(f\"Warning: {np.sum(nan_mask)} NaN values detected in gap calculation\")\n",
        "        print(\"Only using non-NaN values for analysis\")\n",
        "\n",
        "        # NaN   \n",
        "        non_nan_indices = np.where(~np.isnan(gap))[0]\n",
        "        if len(non_nan_indices) == 0:\n",
        "            print(\"Error: All gap values are NaN. Unable to calculate metrics.\")\n",
        "            return 0, False, False\n",
        "\n",
        "        # NaN   \n",
        "        latest_valid_idx = np.max(non_nan_indices)\n",
        "        latest_gap = gap[latest_valid_idx]\n",
        "\n",
        "        # NaN   \n",
        "        avg_gap = np.nanmean(gap)\n",
        "        sum_gap = np.nansum(gap)\n",
        "\n",
        "        # Converging  NaN   \n",
        "        if len(non_nan_indices) > 1:\n",
        "            first_valid_idx = np.min(non_nan_indices)\n",
        "            converging = gap[latest_valid_idx] < gap[first_valid_idx]\n",
        "        else:\n",
        "            converging = False\n",
        "    else:\n",
        "        # NaN    \n",
        "        latest_gap = gap[-1]\n",
        "        avg_gap = np.mean(gap)\n",
        "        sum_gap = np.sum(gap)\n",
        "        converging = gap[-1] < gap[0]\n",
        "\n",
        "    # Determine if curves are stable (low standard deviation in later points)\n",
        "    # NaN    np.nanmean  \n",
        "    late_std_train = np.nanmean(train_std[-3:])\n",
        "    late_std_val = np.nanmean(val_std[-3:])\n",
        "    stable = late_std_train < 0.03 and late_std_val < 0.03\n",
        "\n",
        "    #  \n",
        "    print(f\"Average gap between training and validation: {avg_gap:.4f}\")\n",
        "    print(f\"Latest gap (at maximum training data): {latest_gap:.4f}\")\n",
        "    print(f\"Sum of gaps across all training sizes: {sum_gap:.4f}\")\n",
        "    print(f\"Are curves converging? {'Yes' if converging else 'No'}\")\n",
        "    print(f\"Are curves stable? {'Yes' if stable else 'No'}\")\n",
        "\n",
        "    #  gap   \n",
        "    if latest_gap > 0.1:\n",
        "        print(\" Potential overfitting detected! (latest gap > 0.1)\")\n",
        "    else:\n",
        "        print(\" No significant overfitting detected based on learning curves (latest gap  0.1)\")\n",
        "\n",
        "    # Plot learning curves - NaN   \n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # NaN   \n",
        "    valid_indices = ~np.isnan(train_mean) & ~np.isnan(val_mean)\n",
        "    valid_train_sizes = train_sizes[valid_indices]\n",
        "    valid_train_mean = train_mean[valid_indices]\n",
        "    valid_train_std = train_std[valid_indices]\n",
        "    valid_val_mean = val_mean[valid_indices]\n",
        "    valid_val_std = val_std[valid_indices]\n",
        "\n",
        "    plt.plot(valid_train_sizes, valid_train_mean, 'o-', color='#3498db', label='Training score')\n",
        "    plt.fill_between(valid_train_sizes, valid_train_mean - valid_train_std,\n",
        "                    valid_train_mean + valid_train_std, alpha=0.1, color='#3498db')\n",
        "\n",
        "    plt.plot(valid_train_sizes, valid_val_mean, 'o-', color='#2ecc71', label='Cross-validation score')\n",
        "    plt.fill_between(valid_train_sizes, valid_val_mean - valid_val_std,\n",
        "                    valid_val_mean + valid_val_std, alpha=0.1, color='#2ecc71')\n",
        "\n",
        "    plt.title('Learning Curves for SVM Model')\n",
        "    plt.xlabel('Training Examples')\n",
        "    plt.ylabel('Accuracy Score')\n",
        "    plt.grid(True)\n",
        "    plt.legend(loc='best')\n",
        "\n",
        "    #   -  \n",
        "    plt.annotate(\n",
        "        f'Latest Gap: {latest_gap:.4f}\\nAvg Gap: {avg_gap:.4f}',\n",
        "        xy=(0.5, 0.5), xycoords='axes fraction',\n",
        "        bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"#f8f9fa\", ec=\"#343a40\", alpha=0.8)\n",
        "    )\n",
        "\n",
        "    #   -  \n",
        "    # last_idx = len(valid_train_sizes) - 1\n",
        "    # last_train_size = valid_train_sizes[last_idx]\n",
        "    # last_train_score = valid_train_mean[last_idx]\n",
        "    # last_val_score = valid_val_mean[last_idx]\n",
        "    # plt.plot([last_train_size, last_train_size], [last_val_score, last_train_score],\n",
        "    #         'r--', linewidth=2)\n",
        "    # plt.annotate(\n",
        "    #     f'Latest Gap: {latest_gap:.4f}',\n",
        "    #     xy=(last_train_size, (last_val_score + last_train_score)/2),\n",
        "    #     xytext=(last_train_size-0.15, (last_val_score + last_train_score)/2),\n",
        "    #     arrowprops=dict(arrowstyle=\"->\", color='red'),\n",
        "    #     color='red'\n",
        "    # )\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    #  gap  \n",
        "    return latest_gap, converging, stable\n",
        "\n",
        "\n",
        "# ========================= 3. ROC Analysis and AUC Comparison =========================\n",
        "def perform_roc_analysis():\n",
        "    print(\"\\n3. ROC ANALYSIS AND AUC COMPARISON\")\n",
        "\n",
        "    # Get the number of classes\n",
        "    n_classes = len(np.unique(y))\n",
        "\n",
        "    # Binarize the output for multi-class ROC analysis\n",
        "    y_train_bin = label_binarize(y_train, classes=sorted(np.unique(y)))\n",
        "    y_test_bin = label_binarize(y_test, classes=sorted(np.unique(y)))\n",
        "\n",
        "    # Get probability predictions\n",
        "    y_train_score = best_model.predict_proba(X_train)\n",
        "    y_test_score = best_model.predict_proba(X_test)\n",
        "\n",
        "    # Calculate ROC curve and AUC for each class\n",
        "    fpr_train = dict()\n",
        "    tpr_train = dict()\n",
        "    roc_auc_train = dict()\n",
        "\n",
        "    fpr_test = dict()\n",
        "    tpr_test = dict()\n",
        "    roc_auc_test = dict()\n",
        "\n",
        "    for i in range(n_classes):\n",
        "        fpr_train[i], tpr_train[i], _ = roc_curve(y_train_bin[:, i], y_train_score[:, i])\n",
        "        roc_auc_train[i] = auc(fpr_train[i], tpr_train[i])\n",
        "\n",
        "        fpr_test[i], tpr_test[i], _ = roc_curve(y_test_bin[:, i], y_test_score[:, i])\n",
        "        roc_auc_test[i] = auc(fpr_test[i], tpr_test[i])\n",
        "\n",
        "    # Calculate macro-average AUC\n",
        "    macro_auc_train = np.mean([roc_auc_train[i] for i in range(n_classes)])\n",
        "    macro_auc_test = np.mean([roc_auc_test[i] for i in range(n_classes)])\n",
        "    auc_gap = macro_auc_train - macro_auc_test\n",
        "\n",
        "    print(f\"Training Set Macro-AUC: {macro_auc_train:.4f}\")\n",
        "    print(f\"Test Set Macro-AUC: {macro_auc_test:.4f}\")\n",
        "    print(f\"AUC Gap (): {auc_gap:.4f}\")\n",
        "\n",
        "    if auc_gap > 0.05:\n",
        "        print(\" Potential overfitting detected! (AUC > 0.05)\")\n",
        "    else:\n",
        "        print(\" No significant overfitting detected based on AUC gap (AUC  0.05)\")\n",
        "\n",
        "    # Plot ROC curves\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # Plot macro-average ROC curve for training\n",
        "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "\n",
        "    # Plot class-specific ROC curves\n",
        "    colors = plt.cm.tab10(np.linspace(0, 1, n_classes))\n",
        "\n",
        "    for i, color in zip(range(n_classes), colors):\n",
        "        plt.plot(fpr_train[i], tpr_train[i], color=color, lw=2, linestyle='-',\n",
        "                 label=f'Train ROC class {i+1} (AUC = {roc_auc_train[i]:.3f})')\n",
        "\n",
        "        plt.plot(fpr_test[i], tpr_test[i], color=color, lw=2, linestyle='--',\n",
        "                 label=f'Test ROC class {i+1} (AUC = {roc_auc_test[i]:.3f})')\n",
        "\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curves for Training and Test Sets')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "\n",
        "    # Add annotation for the gap\n",
        "    plt.annotate(f'Macro AUC Gap: {auc_gap:.4f}',\n",
        "                xy=(0.5, 0.2), xycoords='axes fraction',\n",
        "                bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"#f8f9fa\", ec=\"#343a40\", alpha=0.8))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Bar chart comparing class-specific AUCs\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    class_labels = [f'Class {i+1}' for i in range(n_classes)]\n",
        "    x = np.arange(len(class_labels))\n",
        "    width = 0.35\n",
        "\n",
        "    train_aucs = [roc_auc_train[i] for i in range(n_classes)]\n",
        "    test_aucs = [roc_auc_test[i] for i in range(n_classes)]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    bars1 = ax.bar(x - width/2, train_aucs, width, label='Training', color='#3498db')\n",
        "    bars2 = ax.bar(x + width/2, test_aucs, width, label='Test', color='#2ecc71')\n",
        "\n",
        "    ax.set_ylim(0, 1.1)\n",
        "    ax.set_ylabel('AUC')\n",
        "    ax.set_title('Class-Specific AUC Comparison')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(class_labels)\n",
        "    ax.legend()\n",
        "\n",
        "    # Add AUC gap annotations\n",
        "    for i in range(n_classes):\n",
        "        gap = train_aucs[i] - test_aucs[i]\n",
        "        ax.annotate(f'={gap:.3f}',\n",
        "                   xy=(i, max(train_aucs[i], test_aucs[i]) + 0.05),\n",
        "                   ha='center', va='bottom',\n",
        "                   color='red' if gap > 0.05 else 'green')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return macro_auc_train, macro_auc_test, auc_gap\n",
        "\n",
        "# ========================= 4. Class-Specific Metrics Analysis =========================\n",
        "def analyze_class_metrics():\n",
        "    print(\"\\n4. CLASS-SPECIFIC METRICS ANALYSIS\")\n",
        "\n",
        "    # Get predictions\n",
        "    y_train_pred = best_model.predict(X_train)\n",
        "    y_test_pred = best_model.predict(X_test)\n",
        "\n",
        "    # Calculate metrics for training set\n",
        "    precision_train, recall_train, f1_train, _ = precision_recall_fscore_support(y_train, y_train_pred, average=None)\n",
        "\n",
        "    # Calculate metrics for test set\n",
        "    precision_test, recall_test, f1_test, _ = precision_recall_fscore_support(y_test, y_test_pred, average=None)\n",
        "\n",
        "    # Calculate differences\n",
        "    precision_diff = precision_train - precision_test\n",
        "    recall_diff = recall_train - recall_test\n",
        "    f1_diff = f1_train - f1_test\n",
        "\n",
        "    # Check for overfitting in each class\n",
        "    classes = sorted(np.unique(y))\n",
        "\n",
        "    # Print class-specific metrics\n",
        "    print(\"\\nClass-specific metrics differences (Training - Test):\")\n",
        "    print(\"Class\\tPrecision \\tRecall \\tF1 \")\n",
        "\n",
        "    for i, class_label in enumerate(classes):\n",
        "        print(f\"{class_label}\\t{precision_diff[i]:.4f}\\t\\t{recall_diff[i]:.4f}\\t\\t{f1_diff[i]:.4f}\")\n",
        "\n",
        "    # Identify classes with significant overfitting\n",
        "    overfit_classes = []\n",
        "\n",
        "    for i, class_label in enumerate(classes):\n",
        "        if abs(precision_diff[i]) > 0.1 or abs(recall_diff[i]) > 0.1 or abs(f1_diff[i]) > 0.1:\n",
        "            overfit_classes.append(class_label)\n",
        "\n",
        "    if overfit_classes:\n",
        "        print(f\"\\n Potential class-specific overfitting detected in classes: {overfit_classes}\")\n",
        "    else:\n",
        "        print(\"\\n No significant class-specific overfitting detected\")\n",
        "\n",
        "    # Create a heatmap of metric differences\n",
        "    metrics_diff = pd.DataFrame({\n",
        "        'Class': classes,\n",
        "        'Precision ': precision_diff,\n",
        "        'Recall ': recall_diff,\n",
        "        'F1 ': f1_diff\n",
        "    })\n",
        "\n",
        "    metrics_diff_pivot = metrics_diff.melt(id_vars=['Class'], var_name='Metric', value_name='Difference')\n",
        "    pivot_table = metrics_diff_pivot.pivot(index='Class', columns='Metric', values='Difference')\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.heatmap(pivot_table, annot=True, cmap='RdBu_r', center=0, fmt='.3f', cbar_kws={'label': 'Difference (Train - Test)'})\n",
        "    plt.title('Class-Specific Metrics Differences (Training - Test)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return metrics_diff\n",
        "\n",
        "# ========================= 5. Prediction Confidence Analysis =========================\n",
        "def analyze_prediction_confidence():\n",
        "    print(\"\\n5. PREDICTION CONFIDENCE ANALYSIS\")\n",
        "\n",
        "    # Get probability predictions\n",
        "    y_train_proba = best_model.predict_proba(X_train)\n",
        "    y_test_proba = best_model.predict_proba(X_test)\n",
        "\n",
        "    # Get the max probability for each prediction (confidence)\n",
        "    train_confidence = np.max(y_train_proba, axis=1)\n",
        "    test_confidence = np.max(y_test_proba, axis=1)\n",
        "\n",
        "    # Calculate high confidence proportions\n",
        "    high_conf_threshold = 0.8\n",
        "    train_high_conf = np.mean(train_confidence >= high_conf_threshold)\n",
        "    test_high_conf = np.mean(test_confidence >= high_conf_threshold)\n",
        "\n",
        "    print(f\"Proportion of high confidence (>{high_conf_threshold}) predictions:\")\n",
        "    print(f\"Training set: {train_high_conf:.4f}\")\n",
        "    print(f\"Test set: {test_high_conf:.4f}\")\n",
        "    print(f\"Difference: {train_high_conf - test_high_conf:.4f}\")\n",
        "\n",
        "    # Decide if there's a confidence gap indicating overfitting\n",
        "    conf_gap = train_high_conf - test_high_conf\n",
        "\n",
        "    if conf_gap > 0.1:\n",
        "        print(\" Potential overfitting detected! (confidence gap > 0.1)\")\n",
        "    else:\n",
        "        print(\" No significant overfitting detected based on prediction confidence\")\n",
        "\n",
        "    # Plot confidence histograms\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Training set confidence histogram\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.hist(train_confidence, bins=20, alpha=0.7, color='#3498db')\n",
        "    plt.axvline(x=high_conf_threshold, color='r', linestyle='--')\n",
        "    plt.title(f'Training Set Confidence\\nHigh Conf: {train_high_conf:.4f}')\n",
        "    plt.xlabel('Prediction Confidence')\n",
        "    plt.ylabel('Count')\n",
        "\n",
        "    # Test set confidence histogram\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.hist(test_confidence, bins=20, alpha=0.7, color='#2ecc71')\n",
        "    plt.axvline(x=high_conf_threshold, color='r', linestyle='--')\n",
        "    plt.title(f'Test Set Confidence\\nHigh Conf: {test_high_conf:.4f}')\n",
        "    plt.xlabel('Prediction Confidence')\n",
        "    plt.ylabel('Count')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Compare overall confidence distributions\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.kdeplot(train_confidence, label='Training', color='#3498db')\n",
        "    sns.kdeplot(test_confidence, label='Test', color='#2ecc71')\n",
        "    plt.axvline(x=high_conf_threshold, color='r', linestyle='--', label=f'Threshold ({high_conf_threshold})')\n",
        "    plt.title('Prediction Confidence Distribution')\n",
        "    plt.xlabel('Confidence')\n",
        "    plt.ylabel('Density')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return train_high_conf, test_high_conf, conf_gap\n",
        "\n",
        "# ========================= 6. Regularization Parameter Analysis =========================\n",
        "def analyze_regularization_parameter():\n",
        "    print(\"\\n6. REGULARIZATION PARAMETER ANALYSIS\")\n",
        "\n",
        "    # Identify the classifier and preprocessor from the pipeline\n",
        "    preprocessor = best_model.named_steps['preprocessor']\n",
        "    best_C = best_model.named_steps['classifier'].C\n",
        "\n",
        "    print(f\"Current C parameter value: {best_C}\")\n",
        "\n",
        "    # Define a range of C values to test\n",
        "    C_values = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "\n",
        "    # Scores containers\n",
        "    train_scores = []\n",
        "    test_scores = []\n",
        "\n",
        "    for C in C_values:\n",
        "        # Create a new SVM with the current C value\n",
        "        svm = SVC(C=C, probability=True, random_state=42, kernel=best_model.named_steps['classifier'].kernel)\n",
        "\n",
        "        # Create a pipeline with the same preprocessor\n",
        "        pipeline = Pipeline([\n",
        "            ('preprocessor', preprocessor),\n",
        "            ('classifier', svm)\n",
        "        ])\n",
        "\n",
        "        # Fit the model\n",
        "        pipeline.fit(X_train, y_train)\n",
        "\n",
        "        # Get scores\n",
        "        train_score = pipeline.score(X_train, y_train)\n",
        "        test_score = pipeline.score(X_test, y_test)\n",
        "\n",
        "        train_scores.append(train_score)\n",
        "        test_scores.append(test_score)\n",
        "\n",
        "    # Convert C values to strings for plotting\n",
        "    C_values_str = [str(C) for C in C_values]\n",
        "\n",
        "    # Find the index of the best C value in our list\n",
        "    best_C_idx = C_values.index(best_C) if best_C in C_values else -1\n",
        "\n",
        "    # Plot the regularization parameter curve\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(C_values_str, train_scores, 'o-', color='#3498db', label='Training score')\n",
        "    plt.plot(C_values_str, test_scores, 'o-', color='#2ecc71', label='Test score')\n",
        "\n",
        "    # Mark the current C value\n",
        "    if best_C_idx != -1:\n",
        "        plt.axvline(x=best_C_idx, color='r', linestyle='--', label=f'Current C={best_C}')\n",
        "\n",
        "    # Calculate the gaps\n",
        "    gaps = np.array(train_scores) - np.array(test_scores)\n",
        "\n",
        "    # Find the C with minimum gap\n",
        "    min_gap_idx = np.argmin(gaps)\n",
        "    min_gap_C = C_values[min_gap_idx]\n",
        "\n",
        "    # Find the C with maximum test score\n",
        "    max_test_idx = np.argmax(test_scores)\n",
        "    max_test_C = C_values[max_test_idx]\n",
        "\n",
        "    plt.title('Effect of C Parameter on Model Performance')\n",
        "    plt.xlabel('C Parameter (log scale)')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend(loc='best')\n",
        "\n",
        "    # Add annotations\n",
        "    plt.annotate(f'Min Gap C={min_gap_C}',\n",
        "                xy=(C_values_str[min_gap_idx], test_scores[min_gap_idx]),\n",
        "                xytext=(0, 20), textcoords='offset points',\n",
        "                arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3,rad=.2\"))\n",
        "\n",
        "    plt.annotate(f'Max Test Score C={max_test_C}',\n",
        "                xy=(C_values_str[max_test_idx], test_scores[max_test_idx]),\n",
        "                xytext=(0, -30), textcoords='offset points',\n",
        "                arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3,rad=.2\"))\n",
        "\n",
        "    # Add horizontal lines for each gap\n",
        "    for i, (C, gap) in enumerate(zip(C_values_str, gaps)):\n",
        "        if i % 2 == 0:  # Add for every other C value to avoid clutter\n",
        "            plt.plot([i, i], [test_scores[i], train_scores[i]], 'k-', alpha=0.3)\n",
        "            plt.text(i, (test_scores[i] + train_scores[i])/2, f'={gap:.3f}', ha='right')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Recommendations\n",
        "    print(f\"\\nC parameter recommendations:\")\n",
        "    print(f\"- Minimum overfitting gap at C={min_gap_C}\")\n",
        "    print(f\"- Maximum test score at C={max_test_C}\")\n",
        "\n",
        "    if best_C != min_gap_C and best_C != max_test_C:\n",
        "        if min_gap_C == max_test_C:\n",
        "            print(f\" Consider changing C from {best_C} to {min_gap_C} for better generalization and performance\")\n",
        "        else:\n",
        "            print(f\" Consider changing C from {best_C} to either:\")\n",
        "            print(f\"   - {min_gap_C} for minimum overfitting\")\n",
        "            print(f\"   - {max_test_C} for maximum test performance\")\n",
        "    else:\n",
        "        print(\" Current C parameter is optimal\")\n",
        "\n",
        "    return train_scores, test_scores, C_values, best_C, min_gap_C, max_test_C\n",
        "\n",
        "# ========================= 7. Cross-Validation Stability Assessment =========================\n",
        "def assess_cv_stability():\n",
        "    print(\"\\n7. CROSS-VALIDATION STABILITY ASSESSMENT\")\n",
        "\n",
        "    # Define cross-validation\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    # Get the classifier and preprocessor from the pipeline\n",
        "    classifier = best_model.named_steps['classifier']\n",
        "    preprocessor = best_model.named_steps['preprocessor']\n",
        "\n",
        "    # Create a new pipeline for cross-validation\n",
        "    pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', classifier)\n",
        "    ])\n",
        "\n",
        "    # Perform cross-validation\n",
        "    cv_scores = cross_val_score(pipeline, X, y, cv=cv, scoring='accuracy')\n",
        "\n",
        "    # Calculate stability metrics\n",
        "    cv_mean = np.mean(cv_scores)\n",
        "    cv_std = np.std(cv_scores)\n",
        "    cv_range = np.max(cv_scores) - np.min(cv_scores)\n",
        "\n",
        "    print(f\"Cross-validation scores: {cv_scores}\")\n",
        "    print(f\"Mean: {cv_mean:.4f}\")\n",
        "    print(f\"Standard deviation: {cv_std:.4f}\")\n",
        "    print(f\"Range: {cv_range:.4f}\")\n",
        "\n",
        "    # Evaluate stability\n",
        "    if cv_std > 0.05:\n",
        "        print(\" High variance across folds detected (std > 0.05)\")\n",
        "    else:\n",
        "        print(\" Model is stable across cross-validation folds (std  0.05)\")\n",
        "\n",
        "    # Visualize the cross-validation results\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Plot each fold score\n",
        "    plt.bar(range(1, len(cv_scores) + 1), cv_scores, color='#3498db', alpha=0.7)\n",
        "\n",
        "    # Add mean line\n",
        "    plt.axhline(y=cv_mean, color='r', linestyle='-', label=f'Mean: {cv_mean:.4f}')\n",
        "\n",
        "    # Add std bounds\n",
        "    plt.axhline(y=cv_mean + cv_std, color='r', linestyle='--', alpha=0.5,\n",
        "                label=f'Std: {cv_std:.4f}')\n",
        "    plt.axhline(y=cv_mean - cv_std, color='r', linestyle='--', alpha=0.5)\n",
        "\n",
        "    plt.title('Cross-Validation Stability Assessment')\n",
        "    plt.xlabel('Fold')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xticks(range(1, len(cv_scores) + 1))\n",
        "    plt.ylim(0.5, 1.0)\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return cv_mean, cv_std, cv_range\n",
        "\n",
        "# ========================= MAIN EXECUTION FUNCTION =========================\n",
        "def check_overfitting():\n",
        "    \"\"\"Perform comprehensive overfitting analysis on the model\"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"COMPREHENSIVE OVERFITTING ANALYSIS\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Run all analyses\n",
        "    train_acc, test_acc, acc_gap = check_accuracy_gap()\n",
        "    avg_gap, converging, stable = plot_learning_curves()\n",
        "    macro_auc_train, macro_auc_test, auc_gap = perform_roc_analysis()\n",
        "    metrics_diff = analyze_class_metrics()\n",
        "    train_high_conf, test_high_conf, conf_gap = analyze_prediction_confidence()\n",
        "    train_scores, test_scores, C_values, best_C, min_gap_C, max_test_C = analyze_regularization_parameter()\n",
        "    cv_mean, cv_std, cv_range = assess_cv_stability()\n",
        "\n",
        "    # Generate a summary report\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"OVERFITTING ANALYSIS SUMMARY\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    overfitting_indicators = [\n",
        "        (\"Accuracy Gap\", acc_gap > 0.1, acc_gap),\n",
        "        (\"Learning Curve Gap\", avg_gap > 0.1, avg_gap),\n",
        "        (\"AUC Gap\", auc_gap > 0.05, auc_gap),\n",
        "        (\"Confidence Gap\", conf_gap > 0.1, conf_gap),\n",
        "        (\"Cross-validation Stability\", cv_std > 0.05, cv_std)\n",
        "    ]\n",
        "\n",
        "    # Count how many indicators suggest overfitting\n",
        "    overfitting_count = sum(1 for _, is_overfitting, _ in overfitting_indicators if is_overfitting)\n",
        "\n",
        "    # Print summary table\n",
        "    print(\"\\nOverfitting Indicators:\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"{'Indicator':<25} {'Value':<10} {'Status':<10}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for indicator, is_overfitting, value in overfitting_indicators:\n",
        "        status = \" ISSUE\" if is_overfitting else \" OK\"\n",
        "        print(f\"{indicator:<25} {value:.4f}     {status}\")\n",
        "\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Overall assessment\n",
        "    if overfitting_count == 0:\n",
        "        print(\"\\n OVERALL ASSESSMENT: No significant overfitting detected\")\n",
        "        print(\"The model generalizes well to unseen data.\")\n",
        "    elif overfitting_count <= 2:\n",
        "        print(\"\\n OVERALL ASSESSMENT: Mild overfitting detected\")\n",
        "        print(\"The model shows some signs of overfitting but may still be acceptable depending on the use case.\")\n",
        "    else:\n",
        "        print(\"\\n OVERALL ASSESSMENT: Significant overfitting detected\")\n",
        "        print(\"The model is likely overfitting the training data and may not generalize well.\")\n",
        "\n",
        "    # Recommendations\n",
        "    print(\"\\nRECOMMENDATIONS:\")\n",
        "\n",
        "    if overfitting_count > 0:\n",
        "        print(\"1. Consider adjusting the regularization parameter:\")\n",
        "        print(f\"   - Current C: {best_C}\")\n",
        "        print(f\"   - Recommended C for minimizing overfitting: {min_gap_C}\")\n",
        "        print(f\"   - Recommended C for maximizing test performance: {max_test_C}\")\n",
        "\n",
        "        print(\"\\n2. Other techniques to try:\")\n",
        "        print(\"   - Increase training data or perform data augmentation\")\n",
        "        print(\"   - Feature selection to reduce model complexity\")\n",
        "        print(\"   - Ensemble methods (e.g., bagging) to reduce variance\")\n",
        "        print(\"   - Try a simpler kernel (e.g., linear if using rbf)\")\n",
        "    else:\n",
        "        print(\"1. The model appears to be well-tuned.\")\n",
        "        print(\"2. You may want to explore if a simpler model could achieve similar performance.\")\n",
        "        print(\"3. Continue monitoring for overfitting when the model is deployed and used on new data.\")\n",
        "\n",
        "# Run the full analysis\n",
        "check_overfitting()"
      ],
      "metadata": {
        "id": "_0TPv6djt5L6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE, ADASYN\n",
        "from collections import Counter\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('final_cleaned_dataset.csv')\n",
        "\n",
        "# Print original class distribution\n",
        "print(\"Original class distribution:\")\n",
        "print(df['PerformanceScore_Encoded'].value_counts())\n",
        "print()\n",
        "\n",
        "# Check for non-numeric columns\n",
        "numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
        "categorical_cols = df.select_dtypes(exclude=['number']).columns.tolist()\n",
        "\n",
        "print(f\"Numeric columns: {len(numeric_cols)}\")\n",
        "print(f\"Categorical columns: {len(categorical_cols)}\")\n",
        "\n",
        "if 'Position' in df.columns and 'Position' not in categorical_cols:\n",
        "    categorical_cols.append('Position')\n",
        "\n",
        "if 'PerformanceScore_Encoded' in numeric_cols:\n",
        "    numeric_cols.remove('PerformanceScore_Encoded')\n",
        "\n",
        "# Remove categorical columns that are already one-hot encoded (True/False values)\n",
        "true_categorical_cols = []\n",
        "for col in categorical_cols:\n",
        "    if df[col].nunique() > 2 or df[col].dtype == 'object':\n",
        "        true_categorical_cols.append(col)\n",
        "\n",
        "# Create preprocessor\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numeric_cols),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), true_categorical_cols)\n",
        "    ])\n",
        "\n",
        "# Separate features and target variable\n",
        "X = df.drop('PerformanceScore_Encoded', axis=1)\n",
        "y = df['PerformanceScore_Encoded']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Apply preprocessing\n",
        "X_train_scaled = preprocessor.fit_transform(X_train)\n",
        "X_test_scaled = preprocessor.transform(X_test)\n",
        "\n",
        "print(f\"Transformed X_train shape: {X_train_scaled.shape}\")\n",
        "\n",
        "print(\"Training set class distribution before oversampling:\")\n",
        "print(Counter(y_train))\n",
        "print()\n",
        "\n",
        "# Function to train SVM and evaluate\n",
        "def train_and_evaluate(X_train, y_train, X_test, y_test, model_name):\n",
        "    # Train an SVM classifier\n",
        "    svm = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True)\n",
        "    svm.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = svm.predict(X_test)\n",
        "\n",
        "    # Evaluate\n",
        "    print(f\"=== {model_name} ===\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print()\n",
        "\n",
        "# 1. Train model without oversampling\n",
        "print(\"Training SVM without oversampling...\")\n",
        "train_and_evaluate(X_train_scaled, y_train, X_test_scaled, y_test, \"No Oversampling\")\n",
        "\n",
        "# 2. Apply SMOTE oversampling\n",
        "print(\"Applying SMOTE oversampling...\")\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
        "print(\"Training set class distribution after SMOTE:\")\n",
        "print(Counter(y_train_smote))\n",
        "print()\n",
        "\n",
        "train_and_evaluate(X_train_smote, y_train_smote, X_test_scaled, y_test, \"SMOTE\")\n",
        "\n",
        "# 3. Apply ADASYN oversampling\n",
        "print(\"Applying ADASYN oversampling...\")\n",
        "adasyn = ADASYN(random_state=42)\n",
        "X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train_scaled, y_train)\n",
        "print(\"Training set class distribution after ADASYN:\")\n",
        "print(Counter(y_train_adasyn))\n",
        "print()\n",
        "\n",
        "train_and_evaluate(X_train_adasyn, y_train_adasyn, X_test_scaled, y_test, \"ADASYN\")\n",
        "\n",
        "# 4. Fine-tuned SVM with SMOTE (to reduce overfitting)\n",
        "print(\"Training fine-tuned SVM with SMOTE...\")\n",
        "svm_tuned = SVC(kernel='rbf', C=0.1, gamma='auto', probability=True)\n",
        "svm_tuned.fit(X_train_smote, y_train_smote)\n",
        "y_pred_tuned = svm_tuned.predict(X_test_scaled)\n",
        "\n",
        "print(\"=== Fine-tuned SVM with SMOTE ===\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_tuned))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_tuned))"
      ],
      "metadata": {
        "id": "uemrpY6x-K5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing with ADASYN**"
      ],
      "metadata": {
        "id": "i4NqcC17CrOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# =====================\n",
        "# Load the oversampled dataset\n",
        "# =====================\n",
        "# You can use either the SMOTE or ADASYN dataset\n",
        "# Uncomment the one you want to use:\n",
        "\n",
        "# dataset_path = 'employee_data_smote.csv'\n",
        "dataset_path = 'employee_data_adasyn.csv'\n",
        "print(f\"Loading dataset from: {dataset_path}\")\n",
        "\n",
        "df = pd.read_csv(dataset_path)\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Class distribution:\")\n",
        "print(df['PerformanceScore_Encoded'].value_counts())\n",
        "print(df['PerformanceScore_Encoded'].value_counts(normalize=True).round(3) * 100)\n",
        "\n",
        "# =====================\n",
        "# Prepare data for training\n",
        "# =====================\n",
        "# Separate target variable\n",
        "y = df['PerformanceScore_Encoded']\n",
        "X = df.drop('PerformanceScore_Encoded', axis=1)\n",
        "\n",
        "# Split data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Testing set shape: {X_test.shape}\")\n",
        "\n",
        "# =====================\n",
        "# Create SVM model with hyperparameter tuning\n",
        "# =====================\n",
        "# Create a pipeline with scaling and SVM\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('svm', SVC(probability=True, random_state=42))\n",
        "])\n",
        "\n",
        "# Define hyperparameter grid to search\n",
        "param_grid = {\n",
        "    'svm__C': [0.1, 0.5, 1.0, 5.0, 10.0],        # Regularization parameter\n",
        "    'svm__gamma': ['scale', 'auto', 0.1, 0.01],  # Kernel coefficient\n",
        "    'svm__kernel': ['rbf', 'linear']             # Kernel type\n",
        "}\n",
        "\n",
        "# Set up cross-validation\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Create grid search with cross-validation\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline, param_grid, cv=cv, scoring='f1_macro',\n",
        "    verbose=1, n_jobs=-1  # Use all available cores\n",
        ")\n",
        "\n",
        "print(\"\\nPerforming grid search for best hyperparameters...\")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters\n",
        "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Get the best model\n",
        "best_svm = grid_search.best_estimator_\n",
        "\n",
        "# =====================\n",
        "# Evaluate the model\n",
        "# =====================\n",
        "# Make predictions on test set\n",
        "y_pred = best_svm.predict(X_test)\n",
        "y_pred_proba = best_svm.predict_proba(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nTest set accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Create confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# =====================\n",
        "# Visualizations\n",
        "# =====================\n",
        "# Plot confusion matrix as heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=np.unique(y),\n",
        "            yticklabels=np.unique(y))\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.savefig('svm_confusion_matrix.png')\n",
        "plt.show()\n",
        "\n",
        "# Plot classification report as heatmap\n",
        "report = classification_report(y_test, y_pred, output_dict=True)\n",
        "report_df = pd.DataFrame(report).T\n",
        "report_df = report_df.drop(['accuracy', 'macro avg', 'weighted avg'], axis=0)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(report_df.iloc[:, :3], annot=True, cmap='YlGnBu', fmt='.3f')\n",
        "plt.title('Classification Report Heatmap')\n",
        "plt.savefig('svm_classification_report.png')\n",
        "plt.show()\n",
        "\n",
        "# =====================\n",
        "# Save the model\n",
        "# =====================\n",
        "import joblib\n",
        "\n",
        "# Save the trained model\n",
        "joblib.dump(best_svm, 'best_svm_model.pkl')\n",
        "print(\"\\nBest SVM model saved as 'best_svm_model.pkl'\")\n",
        "\n",
        "# =====================\n",
        "# Analysis for moderate overfitting\n",
        "# =====================\n",
        "print(\"\\nChecking for potential overfitting:\")\n",
        "\n",
        "# Compare training vs test performance\n",
        "train_pred = best_svm.predict(X_train)\n",
        "train_accuracy = accuracy_score(y_train, train_pred)\n",
        "test_accuracy = accuracy\n",
        "\n",
        "print(f\"Training accuracy: {train_accuracy:.4f}\")\n",
        "print(f\"Testing accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Difference: {train_accuracy - test_accuracy:.4f}\")\n",
        "\n",
        "if train_accuracy - test_accuracy > 0.05:\n",
        "    print(\"WARNING: Potential overfitting detected (difference > 0.05)\")\n",
        "    print(\"Consider using a more restrictive C value or collecting more data.\")\n",
        "else:\n",
        "    print(\"Model does not show signs of significant overfitting.\")\n",
        "\n",
        "# =====================\n",
        "# Feature importance (for linear kernel only)\n",
        "# =====================\n",
        "if 'linear' in str(best_svm.named_steps['svm']):\n",
        "    # Get feature importance for linear SVM\n",
        "    importance = best_svm.named_steps['svm'].coef_\n",
        "\n",
        "    # Create a DataFrame for feature importance\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'Feature': X.columns,\n",
        "        'Importance': np.mean(np.abs(importance), axis=0)\n",
        "    })\n",
        "\n",
        "    # Sort by importance\n",
        "    feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
        "\n",
        "    # Plot top 15 features\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.barplot(x='Importance', y='Feature', data=feature_importance.head(15))\n",
        "    plt.title('Top 15 Most Important Features (Linear SVM)')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('svm_feature_importance.png')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nTop 5 most important features:\")\n",
        "    print(feature_importance.head(5))\n",
        "else:\n",
        "    print(\"\\nFeature importance is only available for linear kernel.\")\n",
        "    print(f\"Current kernel: {best_svm.named_steps['svm'].kernel}\")\n",
        "\n",
        "print(\"\\nSVM training and evaluation completed!\")"
      ],
      "metadata": {
        "id": "HAJ3SHB8BhRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Overfitting Detection for SVM Employee Model\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import learning_curve, StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, roc_curve, auc, precision_recall_fscore_support,\n",
        "    roc_auc_score, classification_report, confusion_matrix\n",
        ")\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "import pickle\n",
        "from google.colab import files\n",
        "import io\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Upload the dataset and model in Google Colab\n",
        "print(\"Upload your CSV dataset first:\")\n",
        "uploaded_data = files.upload()  # Upload the dataset\n",
        "file_name = list(uploaded_data.keys())[0]\n",
        "\n",
        "print(\"\\nNow upload your saved SVM model:\")\n",
        "uploaded_model = files.upload()  # Upload the model\n",
        "model_name = list(uploaded_model.keys())[0]\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(io.BytesIO(uploaded_data[file_name]))\n",
        "\n",
        "# Convert string True/False to boolean for relevant columns\n",
        "string_bool_columns = [col for col in df.columns if df[col].dtype == 'object' and\n",
        "                       set(df[col].unique()).issubset({\"True\", \"False\"})]\n",
        "for col in string_bool_columns:\n",
        "    df[col] = df[col].map({\"True\": True, \"False\": False})\n",
        "\n",
        "# Define features and target variable\n",
        "X = df.drop('PerformanceScore_Encoded', axis=1)\n",
        "y = df['PerformanceScore_Encoded']\n",
        "\n",
        "# Load the trained model\n",
        "with open(model_name, 'rb') as file:\n",
        "    best_model = pickle.load(file)\n",
        "\n",
        "print(\"\\nModel loaded successfully!\")\n",
        "print(f\"Model type: {type(best_model)}\")\n",
        "print(f\"Pipeline steps: {best_model.named_steps.keys()}\")\n",
        "print(f\"SVM kernel: {best_model.named_steps['classifier'].kernel}\")\n",
        "print(f\"SVM C parameter: {best_model.named_steps['classifier'].C}\")\n",
        "print(f\"SVM gamma parameter: {best_model.named_steps['classifier'].gamma if hasattr(best_model.named_steps['classifier'], 'gamma') else 'auto'}\")\n",
        "\n",
        "# Get the original data split that was used for training\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
        "print(f\"Test set size: {X_test.shape[0]} samples, {X_test.shape[1]} features\")\n",
        "\n",
        "# Print class distribution\n",
        "train_class_dist = pd.Series(y_train).value_counts(normalize=True)\n",
        "test_class_dist = pd.Series(y_test).value_counts(normalize=True)\n",
        "print(\"\\nClass distribution:\")\n",
        "print(\"Training set:\")\n",
        "for cls, pct in train_class_dist.items():\n",
        "    print(f\"  Class {cls}: {pct:.2%} ({sum(y_train == cls)} samples)\")\n",
        "print(\"Test set:\")\n",
        "for cls, pct in test_class_dist.items():\n",
        "    print(f\"  Class {cls}: {pct:.2%} ({sum(y_test == cls)} samples)\")\n",
        "\n",
        "# ========================= 1. Comparing Metrics on Training and Test Sets =========================\n",
        "def check_accuracy_gap():\n",
        "    # Get predictions on training and test sets\n",
        "    y_train_pred = best_model.predict(X_train)\n",
        "    y_test_pred = best_model.predict(X_test)\n",
        "\n",
        "    # Calculate accuracies\n",
        "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "    accuracy_gap = train_accuracy - test_accuracy\n",
        "\n",
        "    print(\"\\n1. ACCURACY GAP ANALYSIS\")\n",
        "    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "    print(f\"Accuracy Gap (): {accuracy_gap:.4f}\")\n",
        "\n",
        "    # More detailed analysis\n",
        "    print(\"\\nDetailed Per-Class Accuracy Analysis:\")\n",
        "    classes = sorted(np.unique(y))\n",
        "\n",
        "    print(f\"{'Class':<10} {'Train Acc':<10} {'Test Acc':<10} {'Gap':<10} {'Status':<10}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for cls in classes:\n",
        "        # Calculate per-class accuracy\n",
        "        train_cls_acc = accuracy_score(y_train[y_train == cls], y_train_pred[y_train == cls])\n",
        "        test_cls_acc = accuracy_score(y_test[y_test == cls], y_test_pred[y_test == cls])\n",
        "        cls_gap = train_cls_acc - test_cls_acc\n",
        "\n",
        "        status = \" Issue\" if cls_gap > 0.1 else \" OK\"\n",
        "        print(f\"{cls:<10} {train_cls_acc:.4f}     {test_cls_acc:.4f}     {cls_gap:.4f}     {status}\")\n",
        "\n",
        "    # Add confusion matrices\n",
        "    print(\"\\nTraining Set Confusion Matrix:\")\n",
        "    train_cm = confusion_matrix(y_train, y_train_pred)\n",
        "    print(pd.DataFrame(train_cm, index=classes, columns=classes))\n",
        "\n",
        "    print(\"\\nTest Set Confusion Matrix:\")\n",
        "    test_cm = confusion_matrix(y_test, y_test_pred)\n",
        "    print(pd.DataFrame(test_cm, index=classes, columns=classes))\n",
        "\n",
        "    if accuracy_gap > 0.1:\n",
        "        print(\"\\n Potential overfitting detected! ( > 0.1)\")\n",
        "        print(f\"   The model performs {accuracy_gap:.2%} better on training data compared to test data.\")\n",
        "        print(\"   This suggests the model may have learned noise or patterns specific to the training data.\")\n",
        "    else:\n",
        "        print(\"\\n No significant overfitting detected based on accuracy gap (  0.1)\")\n",
        "        print(f\"   The gap of {accuracy_gap:.2%} is within acceptable limits for generalization.\")\n",
        "\n",
        "    # Visualization of accuracy comparison\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    bars = plt.bar(['Training', 'Test'], [train_accuracy, test_accuracy], color=['#3498db', '#2ecc71'])\n",
        "    plt.ylim(0, 1.1)\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Accuracy Comparison: Training vs Test')\n",
        "\n",
        "    # Add text labels above bars\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                f'{height:.4f}', ha='center', va='bottom')\n",
        "\n",
        "    # Add a horizontal line for the threshold\n",
        "    plt.axhline(y=train_accuracy - 0.1, color='r', linestyle='--', alpha=0.5)\n",
        "    plt.text(1.05, train_accuracy - 0.1, 'Threshold ( = 0.1)', color='r', va='center')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return train_accuracy, test_accuracy, accuracy_gap\n",
        "\n",
        "# ========================= 2. Learning Curves Analysis =========================\n",
        "def plot_learning_curves():\n",
        "    print(\"\\n2. LEARNING CURVES ANALYSIS\")\n",
        "\n",
        "    # Get the pipeline steps from the model\n",
        "    preprocessor = best_model.named_steps['preprocessor']\n",
        "    classifier = best_model.named_steps['classifier']\n",
        "\n",
        "    # Create a new pipeline for learning curve evaluation\n",
        "    pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', classifier)\n",
        "    ])\n",
        "\n",
        "    # Calculate learning curves\n",
        "    train_sizes = np.linspace(0.1, 1.0, 10)\n",
        "    train_sizes, train_scores, val_scores = learning_curve(\n",
        "        pipeline, X, y, train_sizes=train_sizes, cv=5, scoring='accuracy', random_state=42, n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # Calculate mean and standard deviation for training set scores\n",
        "    train_mean = np.mean(train_scores, axis=1)\n",
        "    train_std = np.std(train_scores, axis=1)\n",
        "\n",
        "    # Calculate mean and standard deviation for validation set scores\n",
        "    val_mean = np.mean(val_scores, axis=1)\n",
        "    val_std = np.std(val_scores, axis=1)\n",
        "\n",
        "    # Calculate the gap between training and validation curves\n",
        "    gap = train_mean - val_mean\n",
        "    avg_gap = np.mean(gap)\n",
        "\n",
        "    # Determine if the curves are converging\n",
        "    converging = gap[-1] < gap[0]\n",
        "\n",
        "    # Determine if curves are stable (low standard deviation in later points)\n",
        "    late_std_train = np.mean(train_std[-3:])\n",
        "    late_std_val = np.mean(val_std[-3:])\n",
        "    stable = late_std_train < 0.03 and late_std_val < 0.03\n",
        "\n",
        "    print(f\"Average gap between training and validation: {avg_gap:.4f}\")\n",
        "    print(f\"Are curves converging? {'Yes' if converging else 'No'}\")\n",
        "    print(f\"Are curves stable? {'Yes' if stable else 'No'}\")\n",
        "\n",
        "    # Print detailed data for each point on the learning curve\n",
        "    print(\"\\nDetailed Learning Curve Data:\")\n",
        "    print(f\"{'Training Size':<15} {'Train Score':<15} {'Train Std':<15} {'Val Score':<15} {'Val Std':<15} {'Gap':<15}\")\n",
        "    print(\"-\" * 90)\n",
        "\n",
        "    for i, size in enumerate(train_sizes):\n",
        "        print(f\"{int(size * X_train.shape[0]):<15} {train_mean[i]:.4f}         {train_std[i]:.4f}         {val_mean[i]:.4f}         {val_std[i]:.4f}         {gap[i]:.4f}\")\n",
        "\n",
        "    # Additional insights about what the curves indicate\n",
        "    print(\"\\nLearning Curve Interpretation:\")\n",
        "    if train_mean[-1] > 0.9 and avg_gap > 0.1:\n",
        "        print(\"- High training score with significant gap indicates overfitting\")\n",
        "    elif train_mean[-1] < 0.8 and avg_gap < 0.05:\n",
        "        print(\"- Low scores on both curves with small gap suggests underfitting\")\n",
        "    elif converging and stable:\n",
        "        print(\"- Converging and stable curves indicate good generalization\")\n",
        "\n",
        "    print(f\"- Starting gap (with {int(train_sizes[0] * X_train.shape[0])} samples): {gap[0]:.4f}\")\n",
        "    print(f\"- Ending gap (with {int(train_sizes[-1] * X_train.shape[0])} samples): {gap[-1]:.4f}\")\n",
        "\n",
        "    if avg_gap > 0.1:\n",
        "        print(\"\\n Potential overfitting detected! (avg gap > 0.1)\")\n",
        "        print(\"   The persistent gap between training and validation scores suggests\")\n",
        "        print(\"   the model is not generalizing well to unseen data.\")\n",
        "    else:\n",
        "        print(\"\\n No significant overfitting detected based on learning curves (avg gap  0.1)\")\n",
        "        print(\"   The model seems to generalize well as the gap is acceptable.\")\n",
        "\n",
        "    # Plot learning curves\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_sizes, train_mean, 'o-', color='#3498db', label='Training score')\n",
        "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='#3498db')\n",
        "\n",
        "    plt.plot(train_sizes, val_mean, 'o-', color='#2ecc71', label='Cross-validation score')\n",
        "    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='#2ecc71')\n",
        "\n",
        "    plt.title('Learning Curves for SVM Model')\n",
        "    plt.xlabel('Training Examples')\n",
        "    plt.ylabel('Accuracy Score')\n",
        "    plt.grid(True)\n",
        "    plt.legend(loc='best')\n",
        "\n",
        "    # Add annotation for the gap\n",
        "    plt.annotate(f'Avg Gap: {avg_gap:.4f}',\n",
        "                xy=(0.5, 0.5), xycoords='axes fraction',\n",
        "                bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"#f8f9fa\", ec=\"#343a40\", alpha=0.8))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return avg_gap, converging, stable\n",
        "\n",
        "# ========================= 3. ROC Analysis and AUC Comparison =========================\n",
        "def perform_roc_analysis():\n",
        "    print(\"\\n3. ROC ANALYSIS AND AUC COMPARISON\")\n",
        "\n",
        "    # Get the number of classes\n",
        "    n_classes = len(np.unique(y))\n",
        "\n",
        "    # Binarize the output for multi-class ROC analysis\n",
        "    y_train_bin = label_binarize(y_train, classes=sorted(np.unique(y)))\n",
        "    y_test_bin = label_binarize(y_test, classes=sorted(np.unique(y)))\n",
        "\n",
        "    # Get probability predictions\n",
        "    y_train_score = best_model.predict_proba(X_train)\n",
        "    y_test_score = best_model.predict_proba(X_test)\n",
        "\n",
        "    # Calculate ROC curve and AUC for each class\n",
        "    fpr_train = dict()\n",
        "    tpr_train = dict()\n",
        "    roc_auc_train = dict()\n",
        "\n",
        "    fpr_test = dict()\n",
        "    tpr_test = dict()\n",
        "    roc_auc_test = dict()\n",
        "\n",
        "    # Store optimal thresholds\n",
        "    optimal_thresholds_train = dict()\n",
        "    optimal_thresholds_test = dict()\n",
        "\n",
        "    for i in range(n_classes):\n",
        "        fpr_train[i], tpr_train[i], thresholds_train = roc_curve(y_train_bin[:, i], y_train_score[:, i])\n",
        "        roc_auc_train[i] = auc(fpr_train[i], tpr_train[i])\n",
        "\n",
        "        # Find optimal threshold (closest to top-left corner)\n",
        "        optimal_idx_train = np.argmax(tpr_train[i] - fpr_train[i])\n",
        "        optimal_thresholds_train[i] = thresholds_train[optimal_idx_train]\n",
        "\n",
        "        fpr_test[i], tpr_test[i], thresholds_test = roc_curve(y_test_bin[:, i], y_test_score[:, i])\n",
        "        roc_auc_test[i] = auc(fpr_test[i], tpr_test[i])\n",
        "\n",
        "        # Find optimal threshold (closest to top-left corner)\n",
        "        optimal_idx_test = np.argmax(tpr_test[i] - fpr_test[i])\n",
        "        optimal_thresholds_test[i] = thresholds_test[optimal_idx_test]\n",
        "\n",
        "    # Calculate macro-average AUC\n",
        "    macro_auc_train = np.mean([roc_auc_train[i] for i in range(n_classes)])\n",
        "    macro_auc_test = np.mean([roc_auc_test[i] for i in range(n_classes)])\n",
        "    auc_gap = macro_auc_train - macro_auc_test\n",
        "\n",
        "    print(f\"Training Set Macro-AUC: {macro_auc_train:.4f}\")\n",
        "    print(f\"Test Set Macro-AUC: {macro_auc_test:.4f}\")\n",
        "    print(f\"AUC Gap (): {auc_gap:.4f}\")\n",
        "\n",
        "    # Detailed class-specific ROC analysis\n",
        "    print(\"\\nDetailed Class-Specific ROC Analysis:\")\n",
        "    print(f\"{'Class':<10} {'Train AUC':<12} {'Test AUC':<12} {'AUC Gap':<12} {'Train Threshold':<16} {'Test Threshold':<16} {'Status':<10}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    for i in range(n_classes):\n",
        "        class_gap = roc_auc_train[i] - roc_auc_test[i]\n",
        "        status = \" Issue\" if class_gap > 0.05 else \" OK\"\n",
        "\n",
        "        print(f\"{i:<10} {roc_auc_train[i]:.4f}      {roc_auc_test[i]:.4f}      {class_gap:.4f}      {optimal_thresholds_train[i]:.4f}          {optimal_thresholds_test[i]:.4f}          {status}\")\n",
        "\n",
        "    # Calculate and print optimal sensitivity, specificity at chosen threshold\n",
        "    print(\"\\nOptimal Operating Points (Test Set):\")\n",
        "    for i in range(n_classes):\n",
        "        # We need to use the class-specific thresholds\n",
        "        # The optimal index was already found when calculating optimal_thresholds_test\n",
        "        optimal_idx_test = np.argmax(tpr_test[i] - fpr_test[i])\n",
        "        sensitivity = tpr_test[i][optimal_idx_test]\n",
        "        specificity = 1 - fpr_test[i][optimal_idx_test]\n",
        "\n",
        "        print(f\"Class {i}: Threshold={optimal_thresholds_test[i]:.4f}, Sensitivity={sensitivity:.4f}, Specificity={specificity:.4f}\")\n",
        "\n",
        "    # Interpretation of results\n",
        "    if auc_gap > 0.05:\n",
        "        print(\"\\n Potential overfitting detected! (AUC > 0.05)\")\n",
        "        print(f\"   The model's discriminative ability is {auc_gap:.2%} better on training data\")\n",
        "        print(\"   compared to test data, suggesting it's learning patterns specific to the training set.\")\n",
        "    else:\n",
        "        print(\"\\n No significant overfitting detected based on AUC gap (AUC  0.05)\")\n",
        "        print(f\"   The AUC gap of {auc_gap:.2%} suggests good generalization of the model's discriminative ability.\")\n",
        "\n",
        "    # Plot ROC curves\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # Plot macro-average ROC curve for training\n",
        "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "\n",
        "    # Plot class-specific ROC curves\n",
        "    colors = plt.cm.tab10(np.linspace(0, 1, n_classes))\n",
        "\n",
        "    for i, color in zip(range(n_classes), colors):\n",
        "        plt.plot(fpr_train[i], tpr_train[i], color=color, lw=2, linestyle='-',\n",
        "                 label=f'Train ROC class {i+1} (AUC = {roc_auc_train[i]:.3f})')\n",
        "\n",
        "        plt.plot(fpr_test[i], tpr_test[i], color=color, lw=2, linestyle='--',\n",
        "                 label=f'Test ROC class {i+1} (AUC = {roc_auc_test[i]:.3f})')\n",
        "\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curves for Training and Test Sets')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "\n",
        "    # Add annotation for the gap\n",
        "    plt.annotate(f'Macro AUC Gap: {auc_gap:.4f}',\n",
        "                xy=(0.5, 0.2), xycoords='axes fraction',\n",
        "                bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"#f8f9fa\", ec=\"#343a40\", alpha=0.8))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Bar chart comparing class-specific AUCs\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    class_labels = [f'Class {i+1}' for i in range(n_classes)]\n",
        "    x = np.arange(len(class_labels))\n",
        "    width = 0.35\n",
        "\n",
        "    train_aucs = [roc_auc_train[i] for i in range(n_classes)]\n",
        "    test_aucs = [roc_auc_test[i] for i in range(n_classes)]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    bars1 = ax.bar(x - width/2, train_aucs, width, label='Training', color='#3498db')\n",
        "    bars2 = ax.bar(x + width/2, test_aucs, width, label='Test', color='#2ecc71')\n",
        "\n",
        "    ax.set_ylim(0, 1.1)\n",
        "    ax.set_ylabel('AUC')\n",
        "    ax.set_title('Class-Specific AUC Comparison')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(class_labels)\n",
        "    ax.legend()\n",
        "\n",
        "    # Add AUC gap annotations\n",
        "    for i in range(n_classes):\n",
        "        gap = train_aucs[i] - test_aucs[i]\n",
        "        ax.annotate(f'={gap:.3f}',\n",
        "                   xy=(i, max(train_aucs[i], test_aucs[i]) + 0.05),\n",
        "                   ha='center', va='bottom',\n",
        "                   color='red' if gap > 0.05 else 'green')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return macro_auc_train, macro_auc_test, auc_gap\n",
        "\n",
        "# ========================= 4. Class-Specific Metrics Analysis =========================\n",
        "def analyze_class_metrics():\n",
        "    print(\"\\n4. CLASS-SPECIFIC METRICS ANALYSIS\")\n",
        "\n",
        "    # Get predictions\n",
        "    y_train_pred = best_model.predict(X_train)\n",
        "    y_test_pred = best_model.predict(X_test)\n",
        "\n",
        "    # Calculate metrics for training set\n",
        "    precision_train, recall_train, f1_train, _ = precision_recall_fscore_support(y_train, y_train_pred, average=None)\n",
        "\n",
        "    # Calculate metrics for test set\n",
        "    precision_test, recall_test, f1_test, _ = precision_recall_fscore_support(y_test, y_test_pred, average=None)\n",
        "\n",
        "    # Calculate differences\n",
        "    precision_diff = precision_train - precision_test\n",
        "    recall_diff = recall_train - recall_test\n",
        "    f1_diff = f1_train - f1_test\n",
        "\n",
        "    # Calculate macro and weighted averages\n",
        "    train_report = classification_report(y_train, y_train_pred, output_dict=True)\n",
        "    test_report = classification_report(y_test, y_test_pred, output_dict=True)\n",
        "\n",
        "    # Check for overfitting in each class\n",
        "    classes = sorted(np.unique(y))\n",
        "\n",
        "    # Print class-specific metrics with more details\n",
        "    print(\"\\nDetailed Class-specific Metrics:\")\n",
        "    print(f\"{'Class':<10} {'Metric':<10} {'Train':<10} {'Test':<10} {'Diff':<10} {'Status':<10}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Define a function to determine metric status\n",
        "    def get_status(diff, threshold=0.1):\n",
        "        if diff > threshold:\n",
        "            return \" Issue\"\n",
        "        elif diff < -threshold:\n",
        "            return \" Lower\"\n",
        "        else:\n",
        "            return \" OK\"\n",
        "\n",
        "    # Track problematic classes\n",
        "    overfit_classes = []\n",
        "    underperform_classes = []\n",
        "\n",
        "    for i, cls in enumerate(classes):\n",
        "        # Check precision\n",
        "        p_status = get_status(precision_diff[i])\n",
        "        print(f\"{cls:<10} {'Precision':<10} {precision_train[i]:.4f}    {precision_test[i]:.4f}    {precision_diff[i]:.4f}    {p_status}\")\n",
        "\n",
        "        # Check recall\n",
        "        r_status = get_status(recall_diff[i])\n",
        "        print(f\"{'':<10} {'Recall':<10} {recall_train[i]:.4f}    {recall_test[i]:.4f}    {recall_diff[i]:.4f}    {r_status}\")\n",
        "\n",
        "        # Check F1\n",
        "        f1_status = get_status(f1_diff[i])\n",
        "        print(f\"{'':<10} {'F1-Score':<10} {f1_train[i]:.4f}    {f1_test[i]:.4f}    {f1_diff[i]:.4f}    {f1_status}\")\n",
        "\n",
        "        # Add separator between classes\n",
        "        if i < len(classes) - 1:\n",
        "            print(\"-\" * 60)\n",
        "\n",
        "        # Track if class has overfitting issues\n",
        "        if abs(precision_diff[i]) > 0.1 or abs(recall_diff[i]) > 0.1 or abs(f1_diff[i]) > 0.1:\n",
        "            if precision_diff[i] > 0.1 or recall_diff[i] > 0.1 or f1_diff[i] > 0.1:\n",
        "                overfit_classes.append(cls)\n",
        "            else:\n",
        "                underperform_classes.append(cls)\n",
        "\n",
        "    # Print aggregated metrics\n",
        "    print(\"\\nAggregated Metrics:\")\n",
        "    print(f\"{'Average':<10} {'Metric':<10} {'Train':<10} {'Test':<10} {'Diff':<10} {'Status':<10}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Macro averages\n",
        "    macro_precision_diff = train_report['macro avg']['precision'] - test_report['macro avg']['precision']\n",
        "    macro_recall_diff = train_report['macro avg']['recall'] - test_report['macro avg']['recall']\n",
        "    macro_f1_diff = train_report['macro avg']['f1-score'] - test_report['macro avg']['f1-score']\n",
        "\n",
        "    print(f\"{'Macro':<10} {'Precision':<10} {train_report['macro avg']['precision']:.4f}    {test_report['macro avg']['precision']:.4f}    {macro_precision_diff:.4f}    {get_status(macro_precision_diff)}\")\n",
        "    print(f\"{'':<10} {'Recall':<10} {train_report['macro avg']['recall']:.4f}    {test_report['macro avg']['recall']:.4f}    {macro_recall_diff:.4f}    {get_status(macro_recall_diff)}\")\n",
        "    print(f\"{'':<10} {'F1-Score':<10} {train_report['macro avg']['f1-score']:.4f}    {test_report['macro avg']['f1-score']:.4f}    {macro_f1_diff:.4f}    {get_status(macro_f1_diff)}\")\n",
        "\n",
        "    # Weighted averages\n",
        "    weighted_precision_diff = train_report['weighted avg']['precision'] - test_report['weighted avg']['precision']\n",
        "    weighted_recall_diff = train_report['weighted avg']['recall'] - test_report['weighted avg']['recall']\n",
        "    weighted_f1_diff = train_report['weighted avg']['f1-score'] - test_report['weighted avg']['f1-score']\n",
        "\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"{'Weighted':<10} {'Precision':<10} {train_report['weighted avg']['precision']:.4f}    {test_report['weighted avg']['precision']:.4f}    {weighted_precision_diff:.4f}    {get_status(weighted_precision_diff)}\")\n",
        "    print(f\"{'':<10} {'Recall':<10} {train_report['weighted avg']['recall']:.4f}    {test_report['weighted avg']['recall']:.4f}    {weighted_recall_diff:.4f}    {get_status(weighted_recall_diff)}\")\n",
        "    print(f\"{'':<10} {'F1-Score':<10} {train_report['weighted avg']['f1-score']:.4f}    {test_report['weighted avg']['f1-score']:.4f}    {weighted_f1_diff:.4f}    {get_status(weighted_f1_diff)}\")\n",
        "\n",
        "    # Summary of findings\n",
        "    if overfit_classes:\n",
        "        print(f\"\\n Potential class-specific overfitting detected in classes: {overfit_classes}\")\n",
        "        print(\"   This indicates the model has learned patterns specific to these classes in the training data\")\n",
        "        print(\"   that do not generalize well to the test set.\")\n",
        "\n",
        "    if underperform_classes:\n",
        "        print(f\"\\n Classes performing better on test than training: {underperform_classes}\")\n",
        "        print(\"   This unusual pattern could indicate sampling issues, data leakage, or class imbalance problems.\")\n",
        "\n",
        "    if not overfit_classes and not underperform_classes:\n",
        "        print(\"\\n No significant class-specific overfitting detected\")\n",
        "        print(\"   All classes show consistent performance between training and test sets.\")\n",
        "\n",
        "    # Create a heatmap of metric differences\n",
        "    metrics_diff = pd.DataFrame({\n",
        "        'Class': classes,\n",
        "        'Precision ': precision_diff,\n",
        "        'Recall ': recall_diff,\n",
        "        'F1 ': f1_diff\n",
        "    })\n",
        "\n",
        "    metrics_diff_pivot = metrics_diff.melt(id_vars=['Class'], var_name='Metric', value_name='Difference')\n",
        "    pivot_table = metrics_diff_pivot.pivot(index='Class', columns='Metric', values='Difference')\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.heatmap(pivot_table, annot=True, cmap='RdBu_r', center=0, fmt='.3f', cbar_kws={'label': 'Difference (Train - Test)'})\n",
        "    plt.title('Class-Specific Metrics Differences (Training - Test)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return metrics_diff, overfit_classes, underperform_classes\n",
        "\n",
        "# ========================= 5. Prediction Confidence Analysis =========================\n",
        "def analyze_prediction_confidence():\n",
        "    print(\"\\n5. PREDICTION CONFIDENCE ANALYSIS\")\n",
        "\n",
        "    # Get probability predictions\n",
        "    y_train_proba = best_model.predict_proba(X_train)\n",
        "    y_test_proba = best_model.predict_proba(X_test)\n",
        "\n",
        "    # Get the max probability for each prediction (confidence)\n",
        "    train_confidence = np.max(y_train_proba, axis=1)\n",
        "    test_confidence = np.max(y_test_proba, axis=1)\n",
        "\n",
        "    # Basic statistics\n",
        "    print(\"\\nConfidence Statistics:\")\n",
        "    print(f\"{'Statistic':<15} {'Training':<15} {'Test':<15} {'Difference':<15}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Mean confidence\n",
        "    mean_train_conf = np.mean(train_confidence)\n",
        "    mean_test_conf = np.mean(test_confidence)\n",
        "    print(f\"{'Mean':<15} {mean_train_conf:.4f}        {mean_test_conf:.4f}        {mean_train_conf - mean_test_conf:.4f}\")\n",
        "\n",
        "    # Median confidence\n",
        "    median_train_conf = np.median(train_confidence)\n",
        "    median_test_conf = np.median(test_confidence)\n",
        "    print(f\"{'Median':<15} {median_train_conf:.4f}        {median_test_conf:.4f}        {median_train_conf - median_test_conf:.4f}\")\n",
        "\n",
        "    # Min confidence\n",
        "    min_train_conf = np.min(train_confidence)\n",
        "    min_test_conf = np.min(test_confidence)\n",
        "    print(f\"{'Min':<15} {min_train_conf:.4f}        {min_test_conf:.4f}        {min_train_conf - min_test_conf:.4f}\")\n",
        "\n",
        "    # Max confidence\n",
        "    max_train_conf = np.max(train_confidence)\n",
        "    max_test_conf = np.max(test_confidence)\n",
        "    print(f\"{'Max':<15} {max_train_conf:.4f}        {max_test_conf:.4f}        {max_train_conf - max_test_conf:.4f}\")\n",
        "\n",
        "    # Standard deviation\n",
        "    std_train_conf = np.std(train_confidence)\n",
        "    std_test_conf = np.std(test_confidence)\n",
        "    print(f\"{'Std Dev':<15} {std_train_conf:.4f}        {std_test_conf:.4f}        {std_train_conf - std_test_conf:.4f}\")\n",
        "\n",
        "    # Calculate confidence thresholds\n",
        "    thresholds = [0.5, 0.6, 0.7, 0.8, 0.9, 0.95]\n",
        "\n",
        "    print(\"\\nConfidence Distribution by Threshold:\")\n",
        "    print(f\"{'Threshold':<10} {'Training %':<15} {'Test %':<15} {'Difference':<15} {'Status':<10}\")\n",
        "    print(\"-\" * 65)\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        train_above = np.mean(train_confidence >= threshold)\n",
        "        test_above = np.mean(test_confidence >= threshold)\n",
        "        diff = train_above - test_above\n",
        "\n",
        "        status = \" Issue\" if diff > 0.1 else \" OK\"\n",
        "\n",
        "        print(f\" {threshold:<8} {train_above:.2%}         {test_above:.2%}         {diff:.2%}           {status}\")\n",
        "\n",
        "    # Calculate high confidence proportions\n",
        "    high_conf_threshold = 0.8\n",
        "    train_high_conf = np.mean(train_confidence >= high_conf_threshold)\n",
        "    test_high_conf = np.mean(test_confidence >= high_conf_threshold)\n",
        "\n",
        "    # Class-specific confidence analysis\n",
        "    classes = sorted(np.unique(y))\n",
        "\n",
        "    print(\"\\nClass-Specific Confidence Analysis:\")\n",
        "    print(f\"{'Class':<10} {'Train Mean':<15} {'Test Mean':<15} {'Difference':<15} {'Status':<10}\")\n",
        "    print(\"-\" * 65)\n",
        "\n",
        "    for cls in classes:\n",
        "        # Get indices for this class\n",
        "        train_cls_idx = (y_train == cls)\n",
        "        test_cls_idx = (y_test == cls)\n",
        "\n",
        "        # Calculate mean confidence for the class\n",
        "        train_cls_conf = np.mean(train_confidence[train_cls_idx])\n",
        "        test_cls_conf = np.mean(test_confidence[test_cls_idx])\n",
        "        diff = train_cls_conf - test_cls_conf\n",
        "\n",
        "        status = \" Issue\" if diff > 0.1 else \" OK\"\n",
        "\n",
        "        print(f\"{cls:<10} {train_cls_conf:.4f}        {test_cls_conf:.4f}        {diff:.4f}          {status}\")\n",
        "\n",
        "    # Decision quality analysis\n",
        "    train_pred = best_model.predict(X_train)\n",
        "    test_pred = best_model.predict(X_test)\n",
        "\n",
        "    # Correct prediction confidence\n",
        "    train_correct = (train_pred == y_train)\n",
        "    test_correct = (test_pred == y_test)\n",
        "\n",
        "    # Mean confidence for correct and incorrect predictions\n",
        "    train_correct_conf = np.mean(train_confidence[train_correct])\n",
        "    train_incorrect_conf = np.mean(train_confidence[~train_correct])\n",
        "    test_correct_conf = np.mean(test_confidence[test_correct])\n",
        "    test_incorrect_conf = np.mean(test_confidence[~test_correct])\n",
        "\n",
        "    print(\"\\nConfidence for Correct vs Incorrect Predictions:\")\n",
        "    print(f\"{'Prediction':<12} {'Training':<15} {'Test':<15} {'Difference':<15}\")\n",
        "    print(\"-\" * 57)\n",
        "    print(f\"{'Correct':<12} {train_correct_conf:.4f}        {test_correct_conf:.4f}        {train_correct_conf - test_correct_conf:.4f}\")\n",
        "    print(f\"{'Incorrect':<12} {train_incorrect_conf:.4f}        {test_incorrect_conf:.4f}        {train_incorrect_conf - test_incorrect_conf:.4f}\")\n",
        "    print(f\"{'Difference':<12} {train_correct_conf - train_incorrect_conf:.4f}        {test_correct_conf - test_incorrect_conf:.4f}\")\n",
        "\n",
        "    # Calibration analysis\n",
        "    print(\"\\nCalibration Analysis:\")\n",
        "    n_bins = 10\n",
        "    bin_edges = np.linspace(0, 1, n_bins + 1)\n",
        "    bin_centers = (bin_edges[1:] + bin_edges[:-1]) / 2\n",
        "\n",
        "    # Training set calibration - using histogram approach instead of digitize\n",
        "    train_bin_accuracies = np.zeros(n_bins)\n",
        "    train_bin_counts = np.zeros(n_bins)\n",
        "\n",
        "    # For each confidence score, find the appropriate bin and update counts\n",
        "    for i in range(len(train_confidence)):\n",
        "        # Find bin index using binary search\n",
        "        bin_idx = min(int(train_confidence[i] * n_bins), n_bins - 1)  # Ensure index is within bounds\n",
        "        train_bin_counts[bin_idx] += 1\n",
        "        if train_pred[i] == y_train.iloc[i]:  # Use iloc for pandas Series\n",
        "            train_bin_accuracies[bin_idx] += 1\n",
        "\n",
        "    # Avoid division by zero\n",
        "    train_bin_accuracies = np.divide(train_bin_accuracies, train_bin_counts,\n",
        "                                     out=np.zeros_like(train_bin_accuracies),\n",
        "                                     where=train_bin_counts != 0)\n",
        "\n",
        "    # Test set calibration\n",
        "    test_bin_accuracies = np.zeros(n_bins)\n",
        "    test_bin_counts = np.zeros(n_bins)\n",
        "\n",
        "    # For each confidence score, find the appropriate bin and update counts\n",
        "    for i in range(len(test_confidence)):\n",
        "        # Find bin index using binary search\n",
        "        bin_idx = min(int(test_confidence[i] * n_bins), n_bins - 1)  # Ensure index is within bounds\n",
        "        test_bin_counts[bin_idx] += 1\n",
        "        if test_pred[i] == y_test.iloc[i]:  # Use iloc for pandas Series\n",
        "            test_bin_accuracies[bin_idx] += 1\n",
        "\n",
        "    # Avoid division by zero\n",
        "    test_bin_accuracies = np.divide(test_bin_accuracies, test_bin_counts,\n",
        "                                   out=np.zeros_like(test_bin_accuracies),\n",
        "                                   where=test_bin_counts != 0)\n",
        "\n",
        "    # Print calibration details\n",
        "    print(f\"{'Conf Range':<15} {'Train Acc':<12} {'Test Acc':<12} {'Train Count':<12} {'Test Count':<12}\")\n",
        "    print(\"-\" * 63)\n",
        "\n",
        "    for i in range(n_bins):\n",
        "        bin_start = bin_edges[i]\n",
        "        bin_end = bin_edges[i+1]\n",
        "        train_acc_str = f\"{train_bin_accuracies[i]:.4f}\" if train_bin_counts[i] > 0 else \"N/A\"\n",
        "        test_acc_str = f\"{test_bin_accuracies[i]:.4f}\" if test_bin_counts[i] > 0 else \"N/A\"\n",
        "        print(f\"{bin_start:.1f}-{bin_end:.1f}       {train_acc_str:<12} {test_acc_str:<12} {int(train_bin_counts[i]):<12} {int(test_bin_counts[i]):<12}\")\n",
        "\n",
        "    # Overall confidence gap assessment\n",
        "    conf_gap = train_high_conf - test_high_conf\n",
        "\n",
        "    if conf_gap > 0.1:\n",
        "        print(\"\\n Potential overfitting detected! (confidence gap > 0.1)\")\n",
        "        print(f\"   The model is {conf_gap:.2%} more confident on training data than test data,\")\n",
        "        print(\"   suggesting it may be too certain about patterns specific to the training set.\")\n",
        "    else:\n",
        "        print(\"\\n No significant overfitting detected based on prediction confidence\")\n",
        "        print(f\"   The confidence gap of {conf_gap:.2%} is within acceptable limits.\")\n",
        "\n",
        "    # Plot confidence histograms\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Training set confidence histogram\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.hist(train_confidence, bins=20, alpha=0.7, color='#3498db')\n",
        "    plt.axvline(x=high_conf_threshold, color='r', linestyle='--')\n",
        "    plt.title(f'Training Set Confidence\\nHigh Conf: {train_high_conf:.4f}')\n",
        "    plt.xlabel('Prediction Confidence')\n",
        "    plt.ylabel('Count')\n",
        "\n",
        "    # Test set confidence histogram\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.hist(test_confidence, bins=20, alpha=0.7, color='#2ecc71')\n",
        "    plt.axvline(x=high_conf_threshold, color='r', linestyle='--')\n",
        "    plt.title(f'Test Set Confidence\\nHigh Conf: {test_high_conf:.4f}')\n",
        "    plt.xlabel('Prediction Confidence')\n",
        "    plt.ylabel('Count')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Compare overall confidence distributions\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.kdeplot(train_confidence, label='Training', color='#3498db')\n",
        "    sns.kdeplot(test_confidence, label='Test', color='#2ecc71')\n",
        "    plt.axvline(x=high_conf_threshold, color='r', linestyle='--', label=f'Threshold ({high_conf_threshold})')\n",
        "    plt.title('Prediction Confidence Distribution')\n",
        "    plt.xlabel('Confidence')\n",
        "    plt.ylabel('Density')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot calibration curves\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(bin_centers, train_bin_accuracies, 'o-', color='#3498db', label='Training')\n",
        "    plt.plot(bin_centers, test_bin_accuracies, 'o-', color='#2ecc71', label='Test')\n",
        "    plt.plot([0, 1], [0, 1], 'k--', label='Perfectly calibrated')\n",
        "    plt.xlabel('Mean predicted confidence')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Calibration Curves')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return train_high_conf, test_high_conf, conf_gap\n",
        "\n",
        "# ========================= 6. Regularization Parameter Analysis =========================\n",
        "def analyze_regularization_parameter():\n",
        "    print(\"\\n6. REGULARIZATION PARAMETER ANALYSIS\")\n",
        "\n",
        "    # Identify the classifier and preprocessor from the pipeline\n",
        "    preprocessor = best_model.named_steps['preprocessor']\n",
        "    best_C = best_model.named_steps['classifier'].C\n",
        "\n",
        "    print(f\"Current C parameter value: {best_C}\")\n",
        "    print(f\"Kernel type: {best_model.named_steps['classifier'].kernel}\")\n",
        "\n",
        "    # Define a range of C values to test\n",
        "    C_values = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "\n",
        "    # If best_C is not in our list, add it\n",
        "    if best_C not in C_values:\n",
        "        C_values.append(best_C)\n",
        "        C_values.sort()\n",
        "\n",
        "    # Scores containers\n",
        "    train_scores = []\n",
        "    test_scores = []\n",
        "\n",
        "    # Additional per-class metrics\n",
        "    class_train_f1 = []\n",
        "    class_test_f1 = []\n",
        "\n",
        "    print(\"\\nTesting different C values...\")\n",
        "    print(f\"{'C Value':<10} {'Train Acc':<12} {'Test Acc':<12} {'Gap':<10} {'Status':<10}\")\n",
        "    print(\"-\" * 54)\n",
        "\n",
        "    for C in C_values:\n",
        "        # Create a new SVM with the current C value\n",
        "        svm = SVC(C=C, probability=True, random_state=42, kernel=best_model.named_steps['classifier'].kernel)\n",
        "\n",
        "        # Create a pipeline with the same preprocessor\n",
        "        pipeline = Pipeline([\n",
        "            ('preprocessor', preprocessor),\n",
        "            ('classifier', svm)\n",
        "        ])\n",
        "\n",
        "        # Fit the model\n",
        "        pipeline.fit(X_train, y_train)\n",
        "\n",
        "        # Get scores\n",
        "        train_score = pipeline.score(X_train, y_train)\n",
        "        test_score = pipeline.score(X_test, y_test)\n",
        "        gap = train_score - test_score\n",
        "\n",
        "        train_scores.append(train_score)\n",
        "        test_scores.append(test_score)\n",
        "\n",
        "        # Get class-specific metrics\n",
        "        y_train_pred = pipeline.predict(X_train)\n",
        "        y_test_pred = pipeline.predict(X_test)\n",
        "\n",
        "        # Get per-class F1 scores\n",
        "        _, _, f1_train, _ = precision_recall_fscore_support(y_train, y_train_pred, average=None)\n",
        "        _, _, f1_test, _ = precision_recall_fscore_support(y_test, y_test_pred, average=None)\n",
        "\n",
        "        class_train_f1.append(f1_train)\n",
        "        class_test_f1.append(f1_test)\n",
        "\n",
        "        # Print results for this C value\n",
        "        status = \" Issue\" if gap > 0.1 else \" OK\"\n",
        "        print(f\"{C:<10} {train_score:.4f}       {test_score:.4f}       {gap:.4f}    {status}\")\n",
        "\n",
        "    # Convert C values to strings for plotting\n",
        "    C_values_str = [str(C) for C in C_values]\n",
        "\n",
        "    # Find the index of the best C value in our list\n",
        "    best_C_idx = C_values.index(best_C)\n",
        "\n",
        "    # Calculate the gaps\n",
        "    gaps = np.array(train_scores) - np.array(test_scores)\n",
        "\n",
        "    # Find the C with minimum gap\n",
        "    min_gap_idx = np.argmin(gaps)\n",
        "    min_gap_C = C_values[min_gap_idx]\n",
        "\n",
        "    # Find the C with maximum test score\n",
        "    max_test_idx = np.argmax(test_scores)\n",
        "    max_test_C = C_values[max_test_idx]\n",
        "\n",
        "    # Print detailed class-specific analysis for best C values\n",
        "    print(\"\\nClass-specific F1 scores at key C values:\")\n",
        "    classes = sorted(np.unique(y))\n",
        "\n",
        "    print(f\"\\nCurrent C={best_C}:\")\n",
        "    for i, cls in enumerate(classes):\n",
        "        print(f\"Class {cls}: Train F1={class_train_f1[best_C_idx][i]:.4f}, Test F1={class_test_f1[best_C_idx][i]:.4f}, Gap={class_train_f1[best_C_idx][i] - class_test_f1[best_C_idx][i]:.4f}\")\n",
        "\n",
        "    print(f\"\\nMinimum gap C={min_gap_C}:\")\n",
        "    for i, cls in enumerate(classes):\n",
        "        print(f\"Class {cls}: Train F1={class_train_f1[min_gap_idx][i]:.4f}, Test F1={class_test_f1[min_gap_idx][i]:.4f}, Gap={class_train_f1[min_gap_idx][i] - class_test_f1[min_gap_idx][i]:.4f}\")\n",
        "\n",
        "    print(f\"\\nMaximum test score C={max_test_C}:\")\n",
        "    for i, cls in enumerate(classes):\n",
        "        print(f\"Class {cls}: Train F1={class_train_f1[max_test_idx][i]:.4f}, Test F1={class_test_f1[max_test_idx][i]:.4f}, Gap={class_train_f1[max_test_idx][i] - class_test_f1[max_test_idx][i]:.4f}\")\n",
        "\n",
        "    # Plot the regularization parameter curve\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(C_values_str, train_scores, 'o-', color='#3498db', label='Training score')\n",
        "    plt.plot(C_values_str, test_scores, 'o-', color='#2ecc71', label='Test score')\n",
        "\n",
        "    # Mark the current C value\n",
        "    plt.axvline(x=best_C_idx, color='r', linestyle='--', label=f'Current C={best_C}')\n",
        "\n",
        "    plt.title('Effect of C Parameter on Model Performance')\n",
        "    plt.xlabel('C Parameter (log scale)')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend(loc='best')\n",
        "\n",
        "    # Add annotations\n",
        "    plt.annotate(f'Min Gap C={min_gap_C}',\n",
        "                xy=(C_values_str[min_gap_idx], test_scores[min_gap_idx]),\n",
        "                xytext=(0, 20), textcoords='offset points',\n",
        "                arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3,rad=.2\"))\n",
        "\n",
        "    plt.annotate(f'Max Test Score C={max_test_C}',\n",
        "                xy=(C_values_str[max_test_idx], test_scores[max_test_idx]),\n",
        "                xytext=(0, -30), textcoords='offset points',\n",
        "                arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3,rad=.2\"))\n",
        "\n",
        "    # Add horizontal lines for each gap\n",
        "    for i, (C, gap) in enumerate(zip(C_values_str, gaps)):\n",
        "        if i % 2 == 0:  # Add for every other C value to avoid clutter\n",
        "            plt.plot([i, i], [test_scores[i], train_scores[i]], 'k-', alpha=0.3)\n",
        "            plt.text(i, (test_scores[i] + train_scores[i])/2, f'={gap:.3f}', ha='right')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot class-specific F1 scores for different C values\n",
        "    plt.figure(figsize=(14, 8))\n",
        "\n",
        "    for i, cls in enumerate(classes):\n",
        "        plt.subplot(len(classes), 1, i + 1)\n",
        "\n",
        "        # Extract class-specific F1 scores\n",
        "        cls_train_f1 = [f1[i] for f1 in class_train_f1]\n",
        "        cls_test_f1 = [f1[i] for f1 in class_test_f1]\n",
        "\n",
        "        plt.plot(C_values_str, cls_train_f1, 'o-', color='#3498db', label='Train F1')\n",
        "        plt.plot(C_values_str, cls_test_f1, 'o-', color='#2ecc71', label='Test F1')\n",
        "\n",
        "        plt.axvline(x=best_C_idx, color='r', linestyle='--')\n",
        "        plt.title(f'Class {cls} F1 Score vs C Parameter')\n",
        "        plt.xlabel('C Parameter (log scale)')\n",
        "        plt.ylabel('F1 Score')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Recommendations\n",
        "    print(f\"\\nC parameter recommendations:\")\n",
        "    print(f\"- Minimum overfitting gap at C={min_gap_C}\")\n",
        "    print(f\"  Train accuracy: {train_scores[min_gap_idx]:.4f}, Test accuracy: {test_scores[min_gap_idx]:.4f}, Gap: {gaps[min_gap_idx]:.4f}\")\n",
        "\n",
        "    print(f\"- Maximum test score at C={max_test_C}\")\n",
        "    print(f\"  Train accuracy: {train_scores[max_test_idx]:.4f}, Test accuracy: {test_scores[max_test_idx]:.4f}, Gap: {gaps[max_test_idx]:.4f}\")\n",
        "\n",
        "    print(f\"- Current model uses C={best_C}\")\n",
        "    print(f\"  Train accuracy: {train_scores[best_C_idx]:.4f}, Test accuracy: {test_scores[best_C_idx]:.4f}, Gap: {gaps[best_C_idx]:.4f}\")\n",
        "\n",
        "    if best_C != min_gap_C and best_C != max_test_C:\n",
        "        if min_gap_C == max_test_C:\n",
        "            print(f\"\\n Recommendation: Change C from {best_C} to {min_gap_C} for better generalization and performance\")\n",
        "            print(\"   This would improve both the overfitting gap and test accuracy.\")\n",
        "        else:\n",
        "            print(f\"\\n Recommendation: Consider changing C from {best_C} to either:\")\n",
        "            print(f\"   - {min_gap_C} for minimum overfitting (prioritizes generalization)\")\n",
        "            print(f\"   - {max_test_C} for maximum test performance (prioritizes accuracy)\")\n",
        "    else:\n",
        "        print(\"\\n Current C parameter is optimal\")\n",
        "        if best_C == min_gap_C:\n",
        "            print(\"   Your model is already using the C value with minimum overfitting.\")\n",
        "        else:\n",
        "            print(\"   Your model is already using the C value with maximum test performance.\")\n",
        "\n",
        "    return train_scores, test_scores, C_values, best_C, min_gap_C, max_test_C\n",
        "\n",
        "# ========================= 7. Cross-Validation Stability Assessment =========================\n",
        "def assess_cv_stability():\n",
        "    print(\"\\n7. CROSS-VALIDATION STABILITY ASSESSMENT\")\n",
        "\n",
        "    # Define cross-validation\n",
        "    n_splits = 5\n",
        "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    # Get the classifier and preprocessor from the pipeline\n",
        "    classifier = best_model.named_steps['classifier']\n",
        "    preprocessor = best_model.named_steps['preprocessor']\n",
        "\n",
        "    # Create a new pipeline for cross-validation\n",
        "    pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', classifier)\n",
        "    ])\n",
        "\n",
        "    # Perform cross-validation for accuracy\n",
        "    cv_scores = cross_val_score(pipeline, X, y, cv=cv, scoring='accuracy')\n",
        "\n",
        "    # Calculate stability metrics\n",
        "    cv_mean = np.mean(cv_scores)\n",
        "    cv_std = np.std(cv_scores)\n",
        "    cv_range = np.max(cv_scores) - np.min(cv_scores)\n",
        "    cv_min = np.min(cv_scores)\n",
        "    cv_max = np.max(cv_scores)\n",
        "\n",
        "    print(f\"Cross-validation scores: {cv_scores}\")\n",
        "    print(f\"Mean accuracy: {cv_mean:.4f}\")\n",
        "    print(f\"Standard deviation: {cv_std:.4f}\")\n",
        "    print(f\"Range: {cv_range:.4f}\")\n",
        "    print(f\"Minimum: {cv_min:.4f}\")\n",
        "    print(f\"Maximum: {cv_max:.4f}\")\n",
        "    print(f\"Coefficient of variation: {cv_std/cv_mean:.4f}\")\n",
        "\n",
        "    # Get more detailed metrics for each fold\n",
        "    print(\"\\nDetailed Fold-by-Fold Analysis:\")\n",
        "    print(f\"{'Fold':<6} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1':<10} {'ROC AUC':<10}\")\n",
        "    print(\"-\" * 56)\n",
        "\n",
        "    fold_metrics = []\n",
        "\n",
        "    for i, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n",
        "        X_fold_train, X_fold_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_fold_train, y_fold_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        # Train the model\n",
        "        pipeline.fit(X_fold_train, y_fold_train)\n",
        "\n",
        "        # Get predictions\n",
        "        y_fold_pred = pipeline.predict(X_fold_val)\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(y_fold_val, y_fold_pred)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(y_fold_val, y_fold_pred, average='weighted')\n",
        "\n",
        "        # Get probabilities for AUC\n",
        "        if hasattr(pipeline, \"predict_proba\"):\n",
        "            y_fold_proba = pipeline.predict_proba(X_fold_val)\n",
        "\n",
        "            # For multiclass problems, we use weighted average of one-vs-rest AUCs\n",
        "            if len(np.unique(y)) > 2:\n",
        "                y_fold_bin = label_binarize(y_fold_val, classes=sorted(np.unique(y)))\n",
        "                n_classes = y_fold_bin.shape[1]\n",
        "\n",
        "                auc_scores = []\n",
        "                for cls in range(n_classes):\n",
        "                    if len(np.unique(y_fold_bin[:, cls])) > 1:  # Check if there are both positive and negative samples\n",
        "                        auc_scores.append(roc_auc_score(y_fold_bin[:, cls], y_fold_proba[:, cls]))\n",
        "\n",
        "                roc_auc = np.mean(auc_scores) if auc_scores else 'N/A'\n",
        "            else:\n",
        "                roc_auc = roc_auc_score(y_fold_val, y_fold_proba[:, 1])\n",
        "        else:\n",
        "            roc_auc = 'N/A'\n",
        "\n",
        "        fold_metrics.append({\n",
        "            'fold': i + 1,\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1,\n",
        "            'roc_auc': roc_auc\n",
        "        })\n",
        "\n",
        "        print(f\"{i+1:<6} {accuracy:.4f}     {precision:.4f}     {recall:.4f}     {f1:.4f}     {roc_auc if isinstance(roc_auc, str) else roc_auc:.4f}\")\n",
        "\n",
        "    # Calculate variation in metrics\n",
        "    metrics_df = pd.DataFrame(fold_metrics)\n",
        "\n",
        "    metrics_to_analyze = ['accuracy', 'precision', 'recall', 'f1']\n",
        "    metrics_stats = {}\n",
        "\n",
        "    for metric in metrics_to_analyze:\n",
        "        metric_values = metrics_df[metric]\n",
        "        metrics_stats[metric] = {\n",
        "            'mean': np.mean(metric_values),\n",
        "            'std': np.std(metric_values),\n",
        "            'cv': np.std(metric_values) / np.mean(metric_values)\n",
        "        }\n",
        "\n",
        "    print(\"\\nMetrics Variation Analysis:\")\n",
        "    print(f\"{'Metric':<10} {'Mean':<10} {'Std Dev':<10} {'CV':<10} {'Status':<10}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for metric, stats in metrics_stats.items():\n",
        "        status = \" Issue\" if stats['cv'] > 0.05 else \" OK\"\n",
        "        print(f\"{metric:<10} {stats['mean']:.4f}     {stats['std']:.4f}     {stats['cv']:.4f}     {status}\")\n",
        "\n",
        "    # Analyze class-specific stability\n",
        "    print(\"\\nClass-Specific Stability Analysis:\")\n",
        "\n",
        "    classes = sorted(np.unique(y))\n",
        "    class_f1_scores = {}\n",
        "\n",
        "    for cls in classes:\n",
        "        class_f1_scores[cls] = []\n",
        "\n",
        "    for i, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n",
        "        X_fold_train, X_fold_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_fold_train, y_fold_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        # Train the model\n",
        "        pipeline.fit(X_fold_train, y_fold_train)\n",
        "\n",
        "        # Get predictions\n",
        "        y_fold_pred = pipeline.predict(X_fold_val)\n",
        "\n",
        "        # Calculate class-specific F1 scores\n",
        "        _, _, f1_scores, _ = precision_recall_fscore_support(y_fold_val, y_fold_pred, average=None)\n",
        "\n",
        "        for j, cls in enumerate(classes):\n",
        "            if j < len(f1_scores):\n",
        "                class_f1_scores[cls].append(f1_scores[j])\n",
        "\n",
        "    print(f\"{'Class':<10} {'Mean F1':<10} {'Std Dev':<10} {'CV':<10} {'Status':<10}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    unstable_classes = []\n",
        "\n",
        "    for cls, scores in class_f1_scores.items():\n",
        "        mean_f1 = np.mean(scores)\n",
        "        std_f1 = np.std(scores)\n",
        "        cv_f1 = std_f1 / mean_f1 if mean_f1 > 0 else float('inf')\n",
        "\n",
        "        status = \" Issue\" if cv_f1 > 0.1 else \" OK\"\n",
        "        if cv_f1 > 0.1:\n",
        "            unstable_classes.append(cls)\n",
        "\n",
        "        print(f\"{cls:<10} {mean_f1:.4f}     {std_f1:.4f}     {cv_f1:.4f}     {status}\")\n",
        "\n",
        "    # Evaluate stability\n",
        "    if cv_std > 0.05:\n",
        "        print(\"\\n High variance across folds detected (std > 0.05)\")\n",
        "        print(f\"   This suggests the model's performance is sensitive to the specific data split.\")\n",
        "        print(\"   Consider using techniques like ensembling or more regularization.\")\n",
        "    else:\n",
        "        print(\"\\n Model is stable across cross-validation folds (std  0.05)\")\n",
        "        print(\"   The model consistently performs well across different data splits.\")\n",
        "\n",
        "    if unstable_classes:\n",
        "        print(f\"\\n Unstable class performance detected for classes: {unstable_classes}\")\n",
        "        print(\"   These classes show high variability in F1 scores across folds.\")\n",
        "        print(\"   Consider collecting more data for these classes or applying class-specific techniques.\")\n",
        "\n",
        "    # Visualize the cross-validation results\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Plot each fold score\n",
        "    plt.bar(range(1, len(cv_scores) + 1), cv_scores, color='#3498db', alpha=0.7)\n",
        "\n",
        "    # Add mean line\n",
        "    plt.axhline(y=cv_mean, color='r', linestyle='-', label=f'Mean: {cv_mean:.4f}')\n",
        "\n",
        "    # Add std bounds\n",
        "    plt.axhline(y=cv_mean + cv_std, color='r', linestyle='--', alpha=0.5,\n",
        "                label=f'Std: {cv_std:.4f}')\n",
        "    plt.axhline(y=cv_mean - cv_std, color='r', linestyle='--', alpha=0.5)\n",
        "\n",
        "    plt.title('Cross-Validation Stability Assessment')\n",
        "    plt.xlabel('Fold')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xticks(range(1, len(cv_scores) + 1))\n",
        "    plt.ylim(0.5, 1.0)\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot class-specific performance across folds\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    for i, (cls, scores) in enumerate(class_f1_scores.items()):\n",
        "        plt.plot(range(1, n_splits + 1), scores, 'o-', label=f'Class {cls}')\n",
        "\n",
        "    plt.title('Class-Specific F1 Scores Across CV Folds')\n",
        "    plt.xlabel('Fold')\n",
        "    plt.ylabel('F1 Score')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return cv_mean, cv_std, cv_range, unstable_classes\n",
        "\n",
        "# ========================= MAIN EXECUTION FUNCTION =========================\n",
        "def check_overfitting():\n",
        "    \"\"\"Perform comprehensive overfitting analysis on the model\"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"COMPREHENSIVE OVERFITTING ANALYSIS\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Run all analyses\n",
        "    train_acc, test_acc, acc_gap = check_accuracy_gap()\n",
        "    avg_gap, converging, stable = plot_learning_curves()\n",
        "    macro_auc_train, macro_auc_test, auc_gap = perform_roc_analysis()\n",
        "    metrics_diff, overfit_classes, underperform_classes = analyze_class_metrics()\n",
        "    train_high_conf, test_high_conf, conf_gap = analyze_prediction_confidence()\n",
        "    train_scores, test_scores, C_values, best_C, min_gap_C, max_test_C = analyze_regularization_parameter()\n",
        "    cv_mean, cv_std, cv_range, unstable_classes = assess_cv_stability()\n",
        "\n",
        "    # Generate a summary report\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"OVERFITTING ANALYSIS SUMMARY\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    overfitting_indicators = [\n",
        "        (\"Accuracy Gap\", acc_gap > 0.1, acc_gap),\n",
        "        (\"Learning Curve Gap\", avg_gap > 0.1, avg_gap),\n",
        "        (\"AUC Gap\", auc_gap > 0.05, auc_gap),\n",
        "        (\"Confidence Gap\", conf_gap > 0.1, conf_gap),\n",
        "        (\"Cross-validation Stability\", cv_std > 0.05, cv_std)\n",
        "    ]\n",
        "\n",
        "    # Count how many indicators suggest overfitting\n",
        "    overfitting_count = sum(1 for _, is_overfitting, _ in overfitting_indicators if is_overfitting)\n",
        "\n",
        "    # Print summary table\n",
        "    print(\"\\nOverfitting Indicators:\")\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"{'Indicator':<25} {'Value':<10} {'Status':<10} {'Interpretation':<25}\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    for indicator, is_overfitting, value in overfitting_indicators:\n",
        "        status = \" ISSUE\" if is_overfitting else \" OK\"\n",
        "\n",
        "        # Add interpretations for each indicator\n",
        "        if indicator == \"Accuracy Gap\":\n",
        "            interp = \"Significant gap\" if is_overfitting else \"Acceptable gap\"\n",
        "        elif indicator == \"Learning Curve Gap\":\n",
        "            interp = \"Poor generalization\" if is_overfitting else \"Good generalization\"\n",
        "        elif indicator == \"AUC Gap\":\n",
        "            interp = \"Inconsistent ranking\" if is_overfitting else \"Consistent ranking\"\n",
        "        elif indicator == \"Confidence Gap\":\n",
        "            interp = \"Overconfident on train\" if is_overfitting else \"Well-calibrated\"\n",
        "        elif indicator == \"Cross-validation Stability\":\n",
        "            interp = \"High data sensitivity\" if is_overfitting else \"Stable performance\"\n",
        "\n",
        "        print(f\"{indicator:<25} {value:.4f}     {status:<10} {interp:<25}\")\n",
        "\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    # Class-specific issues\n",
        "    if overfit_classes:\n",
        "        print(f\"\\nClass-specific overfitting detected in: {overfit_classes}\")\n",
        "\n",
        "    if underperform_classes:\n",
        "        print(f\"Classes performing better on test than training: {underperform_classes}\")\n",
        "\n",
        "    if unstable_classes:\n",
        "        print(f\"Classes with unstable CV performance: {unstable_classes}\")\n",
        "\n",
        "    # Overall assessment with more detail\n",
        "    if overfitting_count == 0:\n",
        "        print(\"\\n OVERALL ASSESSMENT: No significant overfitting detected\")\n",
        "        print(\"The model generalizes well to unseen data with consistent performance across metrics.\")\n",
        "        print(f\"Training accuracy: {train_acc:.4f}, Test accuracy: {test_acc:.4f}\")\n",
        "        print(f\"Current C value: {best_C} is appropriate for this model.\")\n",
        "    elif overfitting_count == 1:\n",
        "        print(\"\\n OVERALL ASSESSMENT: Slight overfitting detected\")\n",
        "        print(\"The model shows a minor sign of overfitting but is generally acceptable.\")\n",
        "        print(f\"The main concern is in the {next(indicator for indicator, is_overfitting, _ in overfitting_indicators if is_overfitting)} metric.\")\n",
        "    elif overfitting_count <= 2:\n",
        "        print(\"\\n OVERALL ASSESSMENT: Mild overfitting detected\")\n",
        "        print(\"The model shows some signs of overfitting but may still be acceptable depending on the use case.\")\n",
        "        print(\"Consider adjusting regularization or using a simpler model if performance in production is a concern.\")\n",
        "    else:\n",
        "        print(\"\\n OVERALL ASSESSMENT: Significant overfitting detected\")\n",
        "        print(\"The model is likely overfitting the training data and may not generalize well to new data.\")\n",
        "        print(\"Immediate action is recommended to address these issues before deploying the model.\")\n",
        "\n",
        "    # Recommendations with more detail\n",
        "    print(\"\\nDETAILED RECOMMENDATIONS:\")\n",
        "\n",
        "    if overfitting_count > 0:\n",
        "        print(\"\\n1. Regularization parameter adjustment:\")\n",
        "        print(f\"   - Current C: {best_C}\")\n",
        "        print(f\"   - Recommended C for minimizing overfitting: {min_gap_C}\")\n",
        "        print(f\"   - Recommended C for maximizing test performance: {max_test_C}\")\n",
        "\n",
        "        if best_C > min_gap_C:\n",
        "            print(f\"    Consider decreasing C from {best_C} to {min_gap_C} to reduce model complexity\")\n",
        "            print(f\"     This would change the gap from {acc_gap:.4f} to approximately {min(gaps for _, _, gaps in overfitting_indicators):.4f}\")\n",
        "\n",
        "        print(\"\\n2. Data enhancements:\")\n",
        "        print(\"   - Consider collecting more training examples, especially for unstable or overfit classes\")\n",
        "        print(\"   - Explore data augmentation techniques to artificially increase training data diversity\")\n",
        "        print(\"   - Review feature engineering to ensure features are generalizable\")\n",
        "\n",
        "        print(\"\\n3. Model adjustments:\")\n",
        "        print(\"   - Feature selection: Remove less important features to reduce model complexity\")\n",
        "        print(\"   - Try a simpler kernel (e.g., linear instead of RBF) if currently using a complex one\")\n",
        "        print(\"   - Consider ensemble methods like bagging to reduce variance\")\n",
        "\n",
        "        print(\"\\n4. Class-specific strategies:\")\n",
        "        if overfit_classes:\n",
        "            print(f\"   - For overfit classes {overfit_classes}, consider:\")\n",
        "            print(\"     * Separate models or class weights\")\n",
        "            print(\"     * Additional regularization for these specific classes\")\n",
        "            print(\"     * More thorough data cleaning/preprocessing for these classes\")\n",
        "\n",
        "        if unstable_classes:\n",
        "            print(f\"   - For unstable classes {unstable_classes}, consider:\")\n",
        "            print(\"     * More data collection focused on these classes\")\n",
        "            print(\"     * Cross-validation stratified by these specific classes\")\n",
        "    else:\n",
        "        print(\"1. The model appears to be well-tuned with good generalization properties.\")\n",
        "        print(\"2. Consider if further optimization is needed:\")\n",
        "        print(f\"   - Current accuracy: {test_acc:.4f}\")\n",
        "        print(f\"   - Best possible accuracy from regularization test: {max(test_scores):.4f} (at C={C_values[np.argmax(test_scores)]})\")\n",
        "        print(\"3. Explore if a simpler model could achieve similar performance with lower computational cost.\")\n",
        "        print(\"4. Establish a monitoring system for when the model is deployed to detect performance drift.\")\n",
        "        print(\"5. Document the model's behavior and limitations for stakeholders.\")\n",
        "\n",
        "# Run the full analysis\n",
        "check_overfitting()"
      ],
      "metadata": {
        "id": "jNhhBnxdD1DK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================\n",
        "# SHAP Analysis for SVM Model\n",
        "# =====================\n",
        "import shap\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"\\nPerforming SHAP analysis...\")\n",
        "\n",
        "# Create a wrapper function to avoid SHAP modifying the Pipeline object\n",
        "def model_predict_proba_wrapper(x):\n",
        "    \"\"\"\n",
        "    Wrapper function that handles both DataFrame and numpy array inputs\n",
        "    without modifying the pipeline object\n",
        "    \"\"\"\n",
        "    # Convert to DataFrame if necessary\n",
        "    if isinstance(x, np.ndarray):\n",
        "        x_df = pd.DataFrame(x, columns=X.columns)\n",
        "    else:\n",
        "        x_df = x\n",
        "    return best_svm.predict_proba(x_df)\n",
        "\n",
        "# Create a background dataset for the explainer (using a subset of training data)\n",
        "# Limit the number of background samples for efficiency\n",
        "if len(X_train) > 100:\n",
        "    if hasattr(X_train, 'sample'):  # It's a DataFrame\n",
        "        X_background = X_train.sample(100, random_state=42)\n",
        "    else:  # It's a NumPy array\n",
        "        indices = np.random.RandomState(42).choice(X_train.shape[0], 100, replace=False)\n",
        "        X_background = X_train[indices]\n",
        "else:\n",
        "    X_background = X_train\n",
        "\n",
        "# Calculate SHAP values for a subset of the test data (for efficiency)\n",
        "if len(X_test) > 100:\n",
        "    if hasattr(X_test, 'sample'):  # It's a DataFrame\n",
        "        X_shap = X_test.sample(100, random_state=42)\n",
        "    else:  # It's a NumPy array\n",
        "        indices = np.random.RandomState(42).choice(X_test.shape[0], 100, replace=False)\n",
        "        X_shap = X_test[indices]\n",
        "else:\n",
        "    X_shap = X_test\n",
        "\n",
        "# Initialize SHAP explainer with the wrapper function\n",
        "try:\n",
        "    print(\"Initializing KernelExplainer with wrapper function...\")\n",
        "    explainer = shap.KernelExplainer(model_predict_proba_wrapper, X_background)\n",
        "    shap_values = explainer.shap_values(X_shap)\n",
        "\n",
        "    # Number of classes\n",
        "    n_classes = len(shap_values)\n",
        "    print(f\"Number of classes detected: {n_classes}\")\n",
        "\n",
        "    # Calculate average absolute SHAP values across all classes for overall importance\n",
        "    overall_importance = np.zeros(X.shape[1])\n",
        "    for i in range(n_classes):\n",
        "        overall_importance += np.abs(shap_values[i]).mean(0)\n",
        "\n",
        "    # Create feature importance DataFrame\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'Feature': X.columns,\n",
        "        'SHAP_Importance': overall_importance\n",
        "    })\n",
        "    feature_importance = feature_importance.sort_values('SHAP_Importance', ascending=False)\n",
        "\n",
        "    print(\"\\nTop 10 most important features according to SHAP:\")\n",
        "    print(feature_importance.head(10))\n",
        "\n",
        "    # Create visualizations\n",
        "    # 1. Summary plot for all classes\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    shap.summary_plot(shap_values, X_shap, feature_names=X.columns, show=False)\n",
        "    plt.title('SHAP Summary Plot (All Classes)')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('svm_shap_summary.png')\n",
        "    plt.show()\n",
        "\n",
        "    # 2. Bar plot for overall feature importance\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.barplot(x='SHAP_Importance', y='Feature', data=feature_importance.head(15))\n",
        "    plt.title('SHAP Feature Importance')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('svm_shap_importance_barplot.png')\n",
        "    plt.show()\n",
        "\n",
        "    # 3. Dependence plots for top 3 features\n",
        "    if n_classes == 2:\n",
        "        # Binary classification - use class 1 (positive class)\n",
        "        class_for_plots = 1\n",
        "    else:\n",
        "        # Multi-class - use the class with highest average predicted probability\n",
        "        proba = best_svm.predict_proba(X_shap)\n",
        "        avg_proba = proba.mean(axis=0)\n",
        "        class_for_plots = np.argmax(avg_proba)\n",
        "\n",
        "    print(f\"Using class {class_for_plots} for dependence plots\")\n",
        "\n",
        "    # Create dependence plots for top 3 features\n",
        "    for feature in feature_importance.head(3)['Feature'].values:\n",
        "        feature_idx = list(X.columns).index(feature)\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        shap.dependence_plot(\n",
        "            feature_idx,\n",
        "            shap_values[class_for_plots],\n",
        "            X_shap,\n",
        "            feature_names=X.columns,\n",
        "            show=False\n",
        "        )\n",
        "        plt.title(f'SHAP Dependence Plot - {feature}')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'svm_shap_dependence_{feature.replace(\" \", \"_\")}.png')\n",
        "        plt.show()\n",
        "\n",
        "    # 4. Force plot for specific samples\n",
        "    print(\"\\nCreating SHAP force plots for sample predictions...\")\n",
        "\n",
        "    # Select a few samples with different predictions\n",
        "    n_samples = min(5, len(X_shap))\n",
        "    sample_indices = np.random.choice(len(X_shap), n_samples, replace=False)\n",
        "\n",
        "    for i, idx in enumerate(sample_indices):\n",
        "        # Get actual prediction\n",
        "        if hasattr(X_shap, 'iloc'):  # It's a DataFrame\n",
        "            sample = X_shap.iloc[idx:idx+1]\n",
        "        else:  # It's a NumPy array\n",
        "            sample = X_shap[idx:idx+1]\n",
        "\n",
        "        pred_class = best_svm.predict(sample)[0]\n",
        "        pred_class_idx = np.where(best_svm.classes_ == pred_class)[0][0]\n",
        "\n",
        "        plt.figure(figsize=(14, 4))\n",
        "        shap.force_plot(\n",
        "            explainer.expected_value[pred_class_idx],\n",
        "            shap_values[pred_class_idx][i],\n",
        "            sample if hasattr(X_shap, 'iloc') else pd.DataFrame(sample, columns=X.columns),\n",
        "            feature_names=list(X.columns),\n",
        "            matplotlib=True,\n",
        "            show=False\n",
        "        )\n",
        "        plt.title(f'SHAP Force Plot for Sample {i+1} (Predicted Class: {pred_class})')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'svm_shap_force_plot_sample_{i+1}.png')\n",
        "        plt.show()\n",
        "\n",
        "        # Also print the top 5 contributors for this prediction\n",
        "        feature_contribution = pd.DataFrame({\n",
        "            'Feature': X.columns,\n",
        "            'SHAP_Value': shap_values[pred_class_idx][i]\n",
        "        })\n",
        "        feature_contribution['AbsValue'] = np.abs(feature_contribution['SHAP_Value'])\n",
        "        feature_contribution = feature_contribution.sort_values('AbsValue', ascending=False)\n",
        "\n",
        "        print(f\"\\nTop 5 contributors for Sample {i+1} (Predicted Class: {pred_class}):\")\n",
        "        for _, row in feature_contribution.head(5).iterrows():\n",
        "            direction = \"increases\" if row['SHAP_Value'] > 0 else \"decreases\"\n",
        "            print(f\"  {row['Feature']}: {row['SHAP_Value']:.4f} ({direction} prediction)\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error in main SHAP analysis: {e}\")\n",
        "\n",
        "    # Fallback to a simpler method focusing only on binary classification\n",
        "    try:\n",
        "        print(\"\\nTrying fallback approach with binary classification focus...\")\n",
        "\n",
        "        # For binary classification, focus on class 1 probabilities only\n",
        "        def f(x):\n",
        "            if isinstance(x, np.ndarray):\n",
        "                x_df = pd.DataFrame(x, columns=X.columns)\n",
        "            else:\n",
        "                x_df = x\n",
        "            return best_svm.predict_proba(x_df)[:, 1]  # Only return class 1 probability\n",
        "\n",
        "        # Use kmeans for background data\n",
        "        background = shap.kmeans(X_background, 10)\n",
        "\n",
        "        # Initialize explainer with simplified function\n",
        "        explainer_binary = shap.KernelExplainer(f, background)\n",
        "\n",
        "        # Calculate SHAP values for a small subset\n",
        "        small_sample = X_shap[:min(20, len(X_shap))]\n",
        "        shap_values_binary = explainer_binary.shap_values(small_sample)\n",
        "\n",
        "        # Create summary plot\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        shap.summary_plot(\n",
        "            shap_values_binary,\n",
        "            small_sample,\n",
        "            feature_names=X.columns,\n",
        "            show=False\n",
        "        )\n",
        "        plt.title('SHAP Summary Plot (Binary Classification)')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('svm_shap_summary_binary.png')\n",
        "        plt.show()\n",
        "\n",
        "        # Create bar plot\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        shap.summary_plot(\n",
        "            shap_values_binary,\n",
        "            small_sample,\n",
        "            feature_names=X.columns,\n",
        "            plot_type=\"bar\",\n",
        "            show=False\n",
        "        )\n",
        "        plt.title('SHAP Feature Importance (Binary Classification)')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('svm_shap_importance_bar_binary.png')\n",
        "        plt.show()\n",
        "\n",
        "    except Exception as e2:\n",
        "        print(f\"Error in fallback SHAP analysis: {e2}\")\n",
        "        print(\"\\nResorting to simple feature importance analysis...\")\n",
        "\n",
        "        # If SVM uses linear kernel, we can extract coefficients\n",
        "        if hasattr(best_svm, 'named_steps') and hasattr(best_svm.named_steps['svm'], 'coef_'):\n",
        "            try:\n",
        "                # Get feature importance from SVM coefficients\n",
        "                importance = np.abs(best_svm.named_steps['svm'].coef_).mean(axis=0)\n",
        "\n",
        "                # Create DataFrame for feature importance\n",
        "                feature_imp_df = pd.DataFrame({\n",
        "                    'Feature': X.columns,\n",
        "                    'Importance': importance\n",
        "                }).sort_values('Importance', ascending=False)\n",
        "\n",
        "                # Plot feature importance\n",
        "                plt.figure(figsize=(12, 8))\n",
        "                sns.barplot(x='Importance', y='Feature', data=feature_imp_df.head(15))\n",
        "                plt.title('SVM Feature Importance (from coefficients)')\n",
        "                plt.tight_layout()\n",
        "                plt.savefig('svm_coefficient_importance.png')\n",
        "                plt.show()\n",
        "\n",
        "                print(\"\\nTop 10 features by coefficient magnitude:\")\n",
        "                print(feature_imp_df.head(10))\n",
        "\n",
        "            except Exception as e3:\n",
        "                print(f\"Error extracting SVM coefficients: {e3}\")\n",
        "\n",
        "print(\"\\nSHAP analysis completed.\")"
      ],
      "metadata": {
        "id": "R5DHjCyOHFAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================\n",
        "# Comprehensive SHAP Analysis for All Classes\n",
        "# =====================\n",
        "import shap\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "\n",
        "print(\"\\nPerforming comprehensive SHAP analysis for all classes...\")\n",
        "\n",
        "# Create a wrapper function to avoid SHAP modifying the Pipeline object\n",
        "def model_predict_proba_wrapper(x):\n",
        "    \"\"\"\n",
        "    Wrapper function that handles both DataFrame and numpy array inputs\n",
        "    without modifying the pipeline object\n",
        "    \"\"\"\n",
        "    # Convert to DataFrame if necessary\n",
        "    if isinstance(x, np.ndarray):\n",
        "        x_df = pd.DataFrame(x, columns=X.columns)\n",
        "    else:\n",
        "        x_df = x\n",
        "    return best_svm.predict_proba(x_df)\n",
        "\n",
        "# Create a background dataset for the explainer (using a subset of training data)\n",
        "# Limit the number of background samples for efficiency\n",
        "if len(X_train) > 100:\n",
        "    if hasattr(X_train, 'sample'):  # It's a DataFrame\n",
        "        X_background = X_train.sample(100, random_state=42)\n",
        "    else:  # It's a NumPy array\n",
        "        indices = np.random.RandomState(42).choice(X_train.shape[0], 100, replace=False)\n",
        "        X_background = X_train[indices]\n",
        "else:\n",
        "    X_background = X_train\n",
        "\n",
        "# Calculate SHAP values for a subset of the test data (for efficiency)\n",
        "if len(X_test) > 50:\n",
        "    if hasattr(X_test, 'sample'):  # It's a DataFrame\n",
        "        X_shap = X_test.sample(50, random_state=42)\n",
        "    else:  # It's a NumPy array\n",
        "        indices = np.random.RandomState(42).choice(X_test.shape[0], 50, replace=False)\n",
        "        X_shap = X_test[indices]\n",
        "else:\n",
        "    X_shap = X_test\n",
        "\n",
        "# Try main approach with KernelExplainer\n",
        "try:\n",
        "    print(\"Initializing KernelExplainer with wrapper function...\")\n",
        "    explainer = shap.KernelExplainer(model_predict_proba_wrapper, X_background)\n",
        "    shap_values = explainer.shap_values(X_shap)\n",
        "\n",
        "    # Number of classes\n",
        "    n_classes = len(shap_values)\n",
        "    print(f\"Number of classes detected: {n_classes}\")\n",
        "\n",
        "    # Get class labels if available\n",
        "    if hasattr(best_svm, 'classes_'):\n",
        "        class_labels = best_svm.classes_\n",
        "        print(f\"Class labels: {class_labels}\")\n",
        "    else:\n",
        "        class_labels = range(n_classes)\n",
        "        print(\"Using default class labels (0, 1, ...)\")\n",
        "\n",
        "    # 1. GLOBAL FEATURE IMPORTANCE ACROSS ALL CLASSES\n",
        "    # Calculate overall importance for all features\n",
        "    print(\"\\n=== Overall Feature Importance (All Classes) ===\")\n",
        "    overall_importance = np.zeros(X.shape[1])\n",
        "    for i in range(n_classes):\n",
        "        overall_importance += np.abs(shap_values[i]).mean(0)\n",
        "\n",
        "    # Create feature importance DataFrame\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'Feature': X.columns,\n",
        "        'SHAP_Importance': overall_importance\n",
        "    })\n",
        "    feature_importance = feature_importance.sort_values('SHAP_Importance', ascending=False)\n",
        "\n",
        "    print(\"Top 10 most important features (all classes combined):\")\n",
        "    print(feature_importance.head(10))\n",
        "\n",
        "    # Create bar plot for overall feature importance\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.barplot(x='SHAP_Importance', y='Feature', data=feature_importance.head(15))\n",
        "    plt.title('Overall SHAP Feature Importance (All Classes Combined)')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('svm_shap_importance_overall.png')\n",
        "    plt.show()\n",
        "\n",
        "    # 2. PER-CLASS FEATURE IMPORTANCE\n",
        "    print(\"\\n=== Per-Class Feature Importance ===\")\n",
        "    # Calculate and visualize feature importance for each class\n",
        "    for i in range(n_classes):\n",
        "        # Calculate mean absolute SHAP values for this class\n",
        "        class_importance = np.abs(shap_values[i]).mean(0)\n",
        "\n",
        "        # Create and sort DataFrame\n",
        "        class_importance_df = pd.DataFrame({\n",
        "            'Feature': X.columns,\n",
        "            'SHAP_Importance': class_importance\n",
        "        })\n",
        "        class_importance_df = class_importance_df.sort_values('SHAP_Importance', ascending=False)\n",
        "\n",
        "        print(f\"\\nTop 10 most important features for Class {class_labels[i]}:\")\n",
        "        print(class_importance_df.head(10))\n",
        "\n",
        "        # Bar plot for this class\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        sns.barplot(x='SHAP_Importance', y='Feature', data=class_importance_df.head(15))\n",
        "        plt.title(f'SHAP Feature Importance for Class {class_labels[i]}')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'svm_shap_importance_class_{class_labels[i]}.png')\n",
        "        plt.show()\n",
        "\n",
        "    # 3. SUMMARY PLOTS FOR ALL CLASSES\n",
        "    print(\"\\n=== SHAP Summary Plots ===\")\n",
        "    # Create summary plots for all classes\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    shap.summary_plot(shap_values, X_shap, feature_names=X.columns, show=False)\n",
        "    plt.title('SHAP Summary Plot (All Classes)')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('svm_shap_summary_all_classes.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Create separate summary plots for each class\n",
        "    for i in range(n_classes):\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        shap.summary_plot(shap_values[i], X_shap, feature_names=X.columns, show=False)\n",
        "        plt.title(f'SHAP Summary Plot for Class {class_labels[i]}')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'svm_shap_summary_class_{class_labels[i]}.png')\n",
        "        plt.show()\n",
        "\n",
        "    # 4. DEPENDENCE PLOTS FOR TOP FEATURES PER CLASS\n",
        "    print(\"\\n=== SHAP Dependence Plots ===\")\n",
        "    # For each class, create dependence plots for the top 3 features\n",
        "    for i in range(n_classes):\n",
        "        # Get top features for this class\n",
        "        class_importance = np.abs(shap_values[i]).mean(0)\n",
        "        top_features_idx = np.argsort(-class_importance)[:3]  # Top 3 features\n",
        "        top_features = [X.columns[idx] for idx in top_features_idx]\n",
        "\n",
        "        print(f\"\\nCreating dependence plots for Class {class_labels[i]}'s top features: {top_features}\")\n",
        "\n",
        "        # Create dependence plots for each top feature\n",
        "        for feature_idx in top_features_idx:\n",
        "            feature_name = X.columns[feature_idx]\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            shap.dependence_plot(\n",
        "                feature_idx,\n",
        "                shap_values[i],\n",
        "                X_shap,\n",
        "                feature_names=X.columns,\n",
        "                show=False\n",
        "            )\n",
        "            plt.title(f'SHAP Dependence Plot - {feature_name} for Class {class_labels[i]}')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f'svm_shap_dependence_class_{class_labels[i]}_{feature_name.replace(\" \", \"_\")}.png')\n",
        "            plt.show()\n",
        "\n",
        "    # 5. FORCE PLOTS FOR REPRESENTATIVE SAMPLES OF EACH CLASS\n",
        "    print(\"\\n=== SHAP Force Plots for Representative Samples ===\")\n",
        "\n",
        "    # Get predictions for the examples in X_shap\n",
        "    y_pred = best_svm.predict(X_shap)\n",
        "\n",
        "    # For each class, find examples that are predicted as that class\n",
        "    for i in range(n_classes):\n",
        "        # Find indices of examples predicted as this class\n",
        "        class_indices = np.where(y_pred == class_labels[i])[0]\n",
        "\n",
        "        if len(class_indices) > 0:\n",
        "            # Take up to 2 examples from this class\n",
        "            sample_count = min(2, len(class_indices))\n",
        "\n",
        "            for j in range(sample_count):\n",
        "                idx = class_indices[j]\n",
        "\n",
        "                # Get the sample\n",
        "                if hasattr(X_shap, 'iloc'):  # It's a DataFrame\n",
        "                    sample = X_shap.iloc[idx:idx+1]\n",
        "                else:  # It's a NumPy array\n",
        "                    sample = X_shap[idx:idx+1]\n",
        "\n",
        "                # Create force plot\n",
        "                plt.figure(figsize=(14, 4))\n",
        "                shap.force_plot(\n",
        "                    explainer.expected_value[i],\n",
        "                    shap_values[i][idx],\n",
        "                    sample if hasattr(X_shap, 'iloc') else pd.DataFrame(sample, columns=X.columns),\n",
        "                    feature_names=list(X.columns),\n",
        "                    matplotlib=True,\n",
        "                    show=False\n",
        "                )\n",
        "                plt.title(f'SHAP Force Plot for Class {class_labels[i]} - Sample {j+1}')\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(f'svm_shap_force_plot_class_{class_labels[i]}_sample_{j+1}.png')\n",
        "                plt.show()\n",
        "\n",
        "                # Print top contributors for this example\n",
        "                feature_contribution = pd.DataFrame({\n",
        "                    'Feature': X.columns,\n",
        "                    'SHAP_Value': shap_values[i][idx]\n",
        "                })\n",
        "                feature_contribution['AbsValue'] = np.abs(feature_contribution['SHAP_Value'])\n",
        "                feature_contribution = feature_contribution.sort_values('AbsValue', ascending=False)\n",
        "\n",
        "                print(f\"Top 5 contributors for Class {class_labels[i]} - Sample {j+1}:\")\n",
        "                for _, row in feature_contribution.head(5).iterrows():\n",
        "                    direction = \"increases\" if row['SHAP_Value'] > 0 else \"decreases\"\n",
        "                    print(f\"  {row['Feature']}: {row['SHAP_Value']:.4f} ({direction} probability)\")\n",
        "        else:\n",
        "            print(f\"No samples predicted as Class {class_labels[i]} in the subset.\")\n",
        "\n",
        "    # 6. CLASS SEPARATION ANALYSIS\n",
        "    print(\"\\n=== Class Separation Analysis ===\")\n",
        "\n",
        "    # Create a DataFrame to analyze how each feature separates classes\n",
        "    if n_classes > 1:  # Only meaningful for multi-class\n",
        "        # Calculate mean SHAP value (not absolute) for each class and feature\n",
        "        class_means = []\n",
        "        for i in range(n_classes):\n",
        "            mean_values = shap_values[i].mean(0)\n",
        "            class_means.append(mean_values)\n",
        "\n",
        "        # Find features with the most divergent SHAP values across classes\n",
        "        class_means = np.array(class_means)\n",
        "        feature_divergence = np.var(class_means, axis=0)\n",
        "\n",
        "        # Create DataFrame for divergence\n",
        "        divergence_df = pd.DataFrame({\n",
        "            'Feature': X.columns,\n",
        "            'Class_Divergence': feature_divergence\n",
        "        }).sort_values('Class_Divergence', ascending=False)\n",
        "\n",
        "        print(\"Features that best separate classes (highest variance in SHAP values):\")\n",
        "        print(divergence_df.head(10))\n",
        "\n",
        "        # Bar plot for class divergence\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        sns.barplot(x='Class_Divergence', y='Feature', data=divergence_df.head(15))\n",
        "        plt.title('Features that Best Separate Classes')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('svm_shap_class_separation.png')\n",
        "        plt.show()\n",
        "\n",
        "        # For the top 3 class-separating features, show class comparison\n",
        "        for feature in divergence_df.head(3)['Feature'].values:\n",
        "            feature_idx = list(X.columns).index(feature)\n",
        "\n",
        "            # Create a DataFrame of mean SHAP values for this feature across classes\n",
        "            feature_class_df = pd.DataFrame({\n",
        "                'Class': [f'Class {class_labels[i]}' for i in range(n_classes)],\n",
        "                'Mean_SHAP': [class_means[i][feature_idx] for i in range(n_classes)]\n",
        "            })\n",
        "\n",
        "            # Plot class comparison\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            sns.barplot(x='Class', y='Mean_SHAP', data=feature_class_df)\n",
        "            plt.title(f'Mean SHAP Values for {feature} Across Classes')\n",
        "            plt.axhline(y=0, color='r', linestyle='--')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f'svm_shap_class_comparison_{feature.replace(\" \", \"_\")}.png')\n",
        "            plt.show()\n",
        "\n",
        "    # 7. DECISION PLOTS (if SHAP version supports it)\n",
        "    try:\n",
        "        print(\"\\n=== Decision Plots for Class Comparison ===\")\n",
        "\n",
        "        # For each class, select a clear example\n",
        "        for i in range(n_classes):\n",
        "            # Find indices of examples predicted as this class with high probability\n",
        "            y_proba = best_svm.predict_proba(X_shap)\n",
        "            class_indices = np.where((y_pred == class_labels[i]) & (y_proba[:, i] > 0.7))[0]\n",
        "\n",
        "            if len(class_indices) > 0:\n",
        "                # Take the example with highest probability\n",
        "                idx = class_indices[np.argmax(y_proba[class_indices, i])]\n",
        "\n",
        "                # Create a decision plot if supported\n",
        "                try:\n",
        "                    plt.figure(figsize=(12, 8))\n",
        "                    shap.decision_plot(\n",
        "                        explainer.expected_value[i],\n",
        "                        shap_values[i][idx],\n",
        "                        X_shap.iloc[idx] if hasattr(X_shap, 'iloc') else pd.DataFrame([X_shap[idx]], columns=X.columns).iloc[0],\n",
        "                        feature_names=list(X.columns),\n",
        "                        show=False\n",
        "                    )\n",
        "                    plt.title(f'SHAP Decision Plot for Class {class_labels[i]}')\n",
        "                    plt.tight_layout()\n",
        "                    plt.savefig(f'svm_shap_decision_plot_class_{class_labels[i]}.png')\n",
        "                    plt.show()\n",
        "                except Exception as e:\n",
        "                    print(f\"  Decision plot not supported or failed: {e}\")\n",
        "                    break  # Skip for other classes too\n",
        "    except Exception as e:\n",
        "        print(f\"Decision plots functionality failed: {e}\")\n",
        "\n",
        "    # 8. INTERACTION EFFECTS (if SHAP version supports it)\n",
        "    try:\n",
        "        print(\"\\n=== Feature Interaction Analysis ===\")\n",
        "\n",
        "        # For the most important feature of each class, calculate interaction effects\n",
        "        for i in range(n_classes):\n",
        "            # Get the most important feature for this class\n",
        "            top_feature_idx = np.argmax(np.abs(shap_values[i]).mean(0))\n",
        "            top_feature = X.columns[top_feature_idx]\n",
        "\n",
        "            print(f\"Analyzing interactions for top feature for Class {class_labels[i]}: {top_feature}\")\n",
        "\n",
        "            # Smaller sample for efficiency\n",
        "            small_sample = X_shap[:min(20, len(X_shap))]\n",
        "\n",
        "            try:\n",
        "                # Calculate SHAP interaction values\n",
        "                interaction_values = explainer.shap_interaction_values(small_sample)\n",
        "\n",
        "                # Plot top interactions\n",
        "                plt.figure(figsize=(10, 8))\n",
        "                feature_interactions = np.abs(interaction_values[i][:, top_feature_idx, :]).mean(0)\n",
        "                feature_interaction_df = pd.DataFrame({\n",
        "                    'Feature': X.columns,\n",
        "                    'Interaction_Strength': feature_interactions\n",
        "                }).sort_values('Interaction_Strength', ascending=False)\n",
        "\n",
        "                # Remove self-interaction\n",
        "                feature_interaction_df = feature_interaction_df[feature_interaction_df['Feature'] != top_feature]\n",
        "\n",
        "                sns.barplot(x='Interaction_Strength', y='Feature', data=feature_interaction_df.head(10))\n",
        "                plt.title(f'Top Feature Interactions with {top_feature} for Class {class_labels[i]}')\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(f'svm_shap_interaction_class_{class_labels[i]}_{top_feature.replace(\" \", \"_\")}.png')\n",
        "                plt.show()\n",
        "\n",
        "                # Dependency plot with interactions for the top interaction\n",
        "                if len(feature_interaction_df) > 0:\n",
        "                    top_interaction = feature_interaction_df.iloc[0]['Feature']\n",
        "                    top_interaction_idx = list(X.columns).index(top_interaction)\n",
        "\n",
        "                    plt.figure(figsize=(10, 8))\n",
        "                    shap.dependence_plot(\n",
        "                        (top_feature_idx, top_interaction_idx),\n",
        "                        interaction_values[i],\n",
        "                        small_sample,\n",
        "                        feature_names=X.columns,\n",
        "                        show=False\n",
        "                    )\n",
        "                    plt.title(f'Interaction: {top_feature}  {top_interaction} for Class {class_labels[i]}')\n",
        "                    plt.tight_layout()\n",
        "                    plt.savefig(f'svm_shap_interaction_plot_class_{class_labels[i]}.png')\n",
        "                    plt.show()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  Interaction analysis failed or not supported: {e}\")\n",
        "                break  # Skip for other classes too\n",
        "    except Exception as e:\n",
        "        print(f\"Interaction analysis functionality failed: {e}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error in main SHAP analysis: {e}\")\n",
        "\n",
        "    # Fallback to a simpler method for each class separately\n",
        "    try:\n",
        "        print(\"\\nTrying fallback approach class by class...\")\n",
        "\n",
        "        # Get the number of classes\n",
        "        if hasattr(best_svm, 'classes_'):\n",
        "            class_labels = best_svm.classes_\n",
        "            n_classes = len(class_labels)\n",
        "        else:\n",
        "            # Try to infer from predictions\n",
        "            y_pred = best_svm.predict(X_shap)\n",
        "            class_labels = np.unique(y_pred)\n",
        "            n_classes = len(class_labels)\n",
        "\n",
        "        print(f\"Detected {n_classes} classes: {class_labels}\")\n",
        "\n",
        "        # For each class, create a binary classifier\n",
        "        for i, class_label in enumerate(class_labels):\n",
        "            print(f\"\\nAnalyzing Class {class_label} vs Rest...\")\n",
        "\n",
        "            # Create a function that returns the probability of this class\n",
        "            def f(x):\n",
        "                if isinstance(x, np.ndarray):\n",
        "                    x_df = pd.DataFrame(x, columns=X.columns)\n",
        "                else:\n",
        "                    x_df = x\n",
        "                probs = best_svm.predict_proba(x_df)\n",
        "                return probs[:, i]  # Return probability for this class\n",
        "\n",
        "            # Create background data\n",
        "            background = shap.kmeans(X_background, 10)\n",
        "\n",
        "            # Initialize explainer with class-specific function\n",
        "            binary_explainer = shap.KernelExplainer(f, background)\n",
        "\n",
        "            # Calculate SHAP values for a small sample\n",
        "            small_sample = X_shap[:min(20, len(X_shap))]\n",
        "            shap_values_binary = binary_explainer.shap_values(small_sample)\n",
        "\n",
        "            # Summary plot for this class\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            shap.summary_plot(\n",
        "                shap_values_binary,\n",
        "                small_sample,\n",
        "                feature_names=X.columns,\n",
        "                show=False\n",
        "            )\n",
        "            plt.title(f'SHAP Summary Plot - Class {class_label} vs Rest')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f'svm_shap_summary_class_{class_label}.png')\n",
        "            plt.show()\n",
        "\n",
        "            # Bar plot for this class\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            shap.summary_plot(\n",
        "                shap_values_binary,\n",
        "                small_sample,\n",
        "                feature_names=X.columns,\n",
        "                plot_type=\"bar\",\n",
        "                show=False\n",
        "            )\n",
        "            plt.title(f'SHAP Feature Importance - Class {class_label} vs Rest')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f'svm_shap_importance_bar_class_{class_label}.png')\n",
        "            plt.show()\n",
        "\n",
        "            # Top 10 features for this class\n",
        "            feature_importance = np.abs(shap_values_binary).mean(0)\n",
        "            class_importance_df = pd.DataFrame({\n",
        "                'Feature': X.columns,\n",
        "                'SHAP_Importance': feature_importance\n",
        "            }).sort_values('SHAP_Importance', ascending=False)\n",
        "\n",
        "            print(f\"Top 10 features for Class {class_label}:\")\n",
        "            print(class_importance_df.head(10))\n",
        "\n",
        "    except Exception as e2:\n",
        "        print(f\"Error in fallback class-by-class analysis: {e2}\")\n",
        "        print(\"\\nResorting to simple feature importance analysis...\")\n",
        "\n",
        "        # If SVM uses linear kernel, we can extract coefficients\n",
        "        if hasattr(best_svm, 'named_steps') and hasattr(best_svm.named_steps['svm'], 'coef_'):\n",
        "            try:\n",
        "                # Get feature importance from SVM coefficients\n",
        "                coef = best_svm.named_steps['svm'].coef_\n",
        "\n",
        "                # Check if we have multiple classes (one-vs-rest approach)\n",
        "                if coef.ndim > 1:\n",
        "                    n_classes = coef.shape[0]\n",
        "                    print(f\"Found coefficients for {n_classes} classes\")\n",
        "\n",
        "                    # For each class, show feature importance\n",
        "                    for i in range(n_classes):\n",
        "                        # Get class coefficients\n",
        "                        class_coef = coef[i]\n",
        "                        importance = np.abs(class_coef)\n",
        "\n",
        "                        # Create DataFrame\n",
        "                        feature_imp_df = pd.DataFrame({\n",
        "                            'Feature': X.columns,\n",
        "                            'Coefficient': class_coef,\n",
        "                            'Importance': importance\n",
        "                        }).sort_values('Importance', ascending=False)\n",
        "\n",
        "                        # Plot\n",
        "                        plt.figure(figsize=(12, 8))\n",
        "                        plt.barh(range(len(importance)), importance[np.argsort(importance)], align='center')\n",
        "                        plt.yticks(range(len(importance)), [X.columns[i] for i in np.argsort(importance)])\n",
        "                        plt.title(f'SVM Feature Importance - Class {i}')\n",
        "                        plt.tight_layout()\n",
        "                        plt.savefig(f'svm_coefficient_importance_class_{i}.png')\n",
        "                        plt.show()\n",
        "\n",
        "                        print(f\"\\nTop 10 features for Class {i} (from coefficients):\")\n",
        "                        print(feature_imp_df.head(10))\n",
        "\n",
        "                else:\n",
        "                    # Binary classification\n",
        "                    importance = np.abs(coef[0])\n",
        "\n",
        "                    # Create DataFrame\n",
        "                    feature_imp_df = pd.DataFrame({\n",
        "                        'Feature': X.columns,\n",
        "                        'Coefficient': coef[0],\n",
        "                        'Importance': importance\n",
        "                    }).sort_values('Importance', ascending=False)\n",
        "\n",
        "                    # Plot\n",
        "                    plt.figure(figsize=(12, 8))\n",
        "                    plt.barh(range(len(importance)), importance[np.argsort(importance)], align='center')\n",
        "                    plt.yticks(range(len(importance)), [X.columns[i] for i in np.argsort(importance)])\n",
        "                    plt.title('SVM Feature Importance (from coefficients)')\n",
        "                    plt.tight_layout()\n",
        "                    plt.savefig('svm_coefficient_importance.png')\n",
        "                    plt.show()\n",
        "\n",
        "                    print(\"\\nTop 10 features (from coefficients):\")\n",
        "                    print(feature_imp_df.head(10))\n",
        "\n",
        "            except Exception as e3:\n",
        "                print(f\"Error extracting SVM coefficients: {e3}\")\n",
        "\n",
        "print(\"\\nComprehensive SHAP analysis completed.\")"
      ],
      "metadata": {
        "id": "cLjD4hfWfqhx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}